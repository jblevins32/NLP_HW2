{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a92cf2770d8f3b9",
   "metadata": {
    "collapsed": false,
    "id": "1a92cf2770d8f3b9"
   },
   "source": [
    "# CS 4650/7650 - Natural Language - HW2\n",
    "\n",
    "Georgia Tech, Summer 2025 (Instructor: Kartik Goyal)\n",
    "\n",
    "<p style=\"color: red;\">DEADLINE: July 31, 2025, 11:59 PM</p>\n",
    "\n",
    "<font color='red'> **NOTE THAT HW2 DOES NOT ACCEPT LATE DAYS!** </font><br>\n",
    "\n",
    "This is an assignment on Neural Text Generation. We will be implementing a Encoder-Decoder transformer from scratch, and then use it for Machine Translation from German to English. Through the assignment, we will also explore multiple concepts related to Transformers, such as Multi-Head Attention, Positional Encoding, etc., and decoding techniques like Beam Search, Greedy Sampling, Top-k Sampling, etc.\n",
    "\n",
    "You can refer to the following resources for PyTorch:\n",
    "- A good tutorial on PyTorch: [Video](https://www.youtube.com/watch?v=OIenNRt2bjg)\n",
    "- Detailed Documentation of PyTorch: [Docs](https://pytorch.org/docs/stable/index.html)\n",
    "- Lecture Material on PyTorch and HuggingFace: [GitHub Repository](https://github.com/neelabhsinha/cs7650-gatech-nlp-pytorch-huggingface-tutorial)\n",
    "\n",
    "Refer to all the `TODO` comments in the code for the parts you need to implement. The total points for this assignment are 100 points. The assignment is divided into the following sections:\n",
    "\n",
    "1. Load and Preprocess Data [5 points]\n",
    "2. Transformer Implementation [38 points]\n",
    "3. Training [10 points]\n",
    "4. Decoding/Sampling [27 points + 15 Bonus Points]\n",
    "5. Analysis [20 points]\n",
    "\n",
    "If images are not visible in the notebook, you can find them in the `images` folder in the assignment directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275bab15e9bfca0",
   "metadata": {
    "collapsed": false,
    "id": "c275bab15e9bfca0"
   },
   "source": [
    "## 0. Setup [0 points - Programming]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d18a6bd29be6bb6",
   "metadata": {
    "id": "1d18a6bd29be6bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Check what version of Python is running\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3298303736ddfd3",
   "metadata": {
    "id": "3298303736ddfd3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4de577726884b93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4de577726884b93",
    "outputId": "2ac12d92-23e2-48c9-bae4-393c9c51b687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used -  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jblevins32/anaconda3/envs/NLP_HW0/lib/python3.11/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# assign a torch random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print('Device being used - ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525d748ecf6da459",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "525d748ecf6da459",
    "outputId": "f3600a13-cd71-4885-be64-e5d7ab9a10bd"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# DO NOT CHANGE THIS CELL\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# RUN THIS CELL ONLY IF RUNNING ON COLAB\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      5\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# RUN THIS CELL ONLY IF RUNNING ON COLAB\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# RUN THIS CELL ONLY IF RUNNING ON COLAB\n",
    "\n",
    "%cd \"/content/drive/MyDrive/.../directory_of_this_amazing_NLP_hw\" # Change this to the directory where you have the files of the homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43fb85daeca324e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43fb85daeca324e",
    "outputId": "a82bacbd-057d-48ac-a7eb-066c0bea429b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME\n",
      "/home/jblevins32/scratch/hf_cache\n",
      "TRITON_CACHE_DIR\n",
      "/home/jblevins32/scratch/triton_cache\n",
      "TORCHINDUCTOR_CACHE_DIR\n",
      "/home/jblevins32/scratch/inductor_cache\n",
      "NLTK_DATA\n",
      "/home/jblevins32/scratch/nltk_data\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# RUN THIS CELL ONLY IF RUNNING ON PACE-ICE \n",
    "\n",
    "# You can ignore this cell if you have already changed the symlink of .cache/ to a location in scratch folder\n",
    "\n",
    "\n",
    "# override the huggingface cache path and nltk cache path\n",
    "import os\n",
    "\n",
    "dirs = {\n",
    "    \"HF_HOME\":\"~/scratch/hf_cache\",\n",
    "    \"TRITON_CACHE_DIR\":\"~/scratch/triton_cache\",\n",
    "    \"TORCHINDUCTOR_CACHE_DIR\":\"~/scratch/inductor_cache\",\n",
    "    'NLTK_DATA':\"~/scratch/nltk_data\"\n",
    "}\n",
    "\n",
    "for name in dirs:\n",
    "    d = dirs[name]\n",
    "    path = os.path.expanduser(d)\n",
    "    print(name)\n",
    "    print(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    # making sure the cache dirs are rwx for owner\n",
    "    os.chmod(path, 0o700)\n",
    "    os.environ[name] = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "KGWmR4EBYQPu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGWmR4EBYQPu",
    "outputId": "e5fe4c81-3f53-40fc-c0a8-0bfaa7ff0ff0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/jblevins32/scratch/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55591661d301ca48",
   "metadata": {
    "collapsed": false,
    "id": "55591661d301ca48"
   },
   "source": [
    "## 1. Load and Preprocess Data [5 points - Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24f78d",
   "metadata": {
    "id": "5b24f78d"
   },
   "source": [
    "In this section, you will create DataLoaders for training your transformer model. You are already provided with functions for reading the parallel corpora and building a vocabulary. (Note that we are building *separate* vocabularies for the source and target languages.)\n",
    "\n",
    "Your task is to implement the `Batch` class and `collate_batch` function in `datautils.py`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c7b44a",
   "metadata": {
    "id": "46c7b44a"
   },
   "outputs": [],
   "source": [
    "from datautils import collate_batch, Batch, read_pairs, TranslationDataset, build_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0a3a49",
   "metadata": {
    "id": "8c0a3a49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building de tokens...\n",
      "len of vocab: 16181\n",
      "Building en tokens...\n",
      "len of vocab: 11503\n"
     ]
    }
   ],
   "source": [
    "## LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "DATA = os.path.join(os.getcwd(), \"data\")\n",
    "# load data\n",
    "valid_src_path = os.path.join(DATA, \"valid.de-en.de\")\n",
    "valid_tgt_path = os.path.join(DATA, \"valid.de-en.en\")\n",
    "\n",
    "data = read_pairs(valid_src_path, valid_tgt_path)\n",
    "\n",
    "# create vocab\n",
    "src_data = [sent[0] for sent in data]\n",
    "tgt_data = [sent[1] for sent in data]\n",
    "src_vocab = build_vocab(src_data, \"de\")\n",
    "tgt_vocab = build_vocab(tgt_data, \"en\")\n",
    "\n",
    "# create dataset\n",
    "dataset = TranslationDataset(data, src_vocab, tgt_vocab)\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f2eeb7c283f0f",
   "metadata": {
    "collapsed": false,
    "id": "c18f2eeb7c283f0f"
   },
   "source": [
    "### 1.1. Batch Class [3 points - Programming]\n",
    "\n",
    "The `Batch` class is a simple data structure that holds the source and target sequences, along with the target mask and the number of tokens in the batch. The `Batch` class will be used to store the data for each batch during training.\n",
    "\n",
    "Implement the `Batch` class in `datautils.py`. Most of the implementation is already added by us, you just need to define the correct mask logic. Feel free to do a disjoint section 2.8. before coming back here, as you can use that function here too (you can do this independently as well, it is not a hard constraint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d14376ac",
   "metadata": {
    "id": "d14376ac"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "allclose(): argument 'other' (position 2) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.equal(expected_tgt, batch.tgt), \u001b[33m\"\u001b[39m\u001b[33mTest Failed: Incorrect values in response for tgt.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.equal(expected_tgt_y, batch.tgt_y), \u001b[33m\"\u001b[39m\u001b[33mTest Failed: Incorrect values in response for tgt_y.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected_tgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mTest Failed: Incorrect values in response for tgt_mask.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m expected_ntokens == batch.ntokens.item(), \u001b[33m\"\u001b[39m\u001b[33mTest Failed: Incorrect values in response for ntokens.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBatch - All tests passed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: allclose(): argument 'other' (position 2) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "\n",
    "src = torch.tensor([[ 4699.,  4622., 14840., 10188., 13378.,  7845.,  7015., 11664., 13122.,\n",
    "          2.,  2., 2.]])\n",
    "tgt = torch.tensor([[ 15820.,  37673., 29931., 21901., 13016., 43304., 2., 2.]])\n",
    "\n",
    "batch = Batch(src, tgt)\n",
    "\n",
    "expected_tgt = torch.Tensor([[1.5820e+04, 3.7673e+04, 2.9931e+04, 2.1901e+04, 1.3016e+04, 4.3304e+04,\n",
    "         2.0000e+00]])\n",
    "\n",
    "expected_tgt_y = torch.Tensor([[37673., 29931., 21901., 13016., 43304., 2., 2.]])\n",
    "\n",
    "expected_tgt_mask = torch.Tensor([[\n",
    "    [ True, False, False, False, False, False, False],\n",
    "    [ True,  True, False, False, False, False, False],\n",
    "    [ True,  True,  True, False, False, False, False],\n",
    "    [ True,  True,  True,  True, False, False, False],\n",
    "    [ True,  True,  True,  True,  True, False, False],\n",
    "    [ True,  True,  True,  True,  True,  True, False],\n",
    "    [ True,  True,  True,  True,  True,  True,  False]]]).to(torch.bool)\n",
    "\n",
    "expected_ntokens = 5\n",
    "\n",
    "assert torch.equal(expected_tgt, batch.tgt), \"Test Failed: Incorrect values in response for tgt.\"\n",
    "assert torch.equal(expected_tgt_y, batch.tgt_y), \"Test Failed: Incorrect values in response for tgt_y.\"\n",
    "assert torch.allclose(expected_tgt_mask, batch.tgt_mask), \"Test Failed: Incorrect values in response for tgt_mask.\"\n",
    "assert expected_ntokens == batch.ntokens.item(), \"Test Failed: Incorrect values in response for ntokens.\"\n",
    "print(\"Batch - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a407022b27f5c",
   "metadata": {
    "collapsed": false,
    "id": "8a4a407022b27f5c"
   },
   "source": [
    "### 1.2. Data Collation [2 points - Programming]\n",
    "\n",
    "The `collate_batch` function is responsible for collating a list of samples into a batch. It pads the sequences to the same length, creates the target mask, and computes the number of tokens in the batch. The function returns a `Batch` object containing the source and target sequences, the target mask, and the number of tokens in the batch.\n",
    "\n",
    "Implement the `collate_batch` function in `datautils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b6cb3",
   "metadata": {
    "id": "3c1b6cb3"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "batch = [(\n",
    "    torch.tensor([4699, 4622, 14840, 10188, 13378, 7845, 7015, 11664, 13122, 2896, 4735, 11933]),\n",
    "    torch.tensor([3541, 488, 2238, 9053, 1028, 5458, 1735, 11248, 8474])\n",
    ")]\n",
    "\n",
    "output = collate_batch(batch, device, 16)\n",
    "\n",
    "expected_src = torch.Tensor([[    0,  4699,  4622, 14840, 10188, 13378,  7845,  7015, 11664, 13122,\n",
    "          2896,  4735, 11933,     1,     2,     2]]).to(device).to(torch.long)\n",
    "expected_tgt = torch.Tensor([[    0,  3541,   488,  2238,  9053,  1028,  5458,  1735, 11248,  8474,\n",
    "             1,     2,     2,     2,     2]]).to(device).to(torch.long)\n",
    "expected_tgt_y = torch.Tensor([[ 3541,   488,  2238,  9053,  1028,  5458,  1735, 11248,  8474,     1,\n",
    "             2,     2,     2,     2,     2]]).to(device).to(torch.long)\n",
    "expected_tgt_mask = torch.Tensor(\n",
    "\n",
    ").to(device).to(torch.long)\n",
    "\n",
    "expected_ntokens = 10\n",
    "\n",
    "assert torch.equal(expected_src, output.src), \"Test Failed: Incorrect values in response for src.\"\n",
    "assert torch.equal(expected_tgt, output.tgt), \"Test Failed: Incorrect values in response for tgt.\"\n",
    "assert torch.equal(expected_tgt_y, output.tgt_y), \"Test Failed: Incorrect values in response for tgt_y.\"\n",
    "# assert torch.allclose(expected_tgt_mask, output.tgt_mask), \"Test Failed: Incorrect values in response for tgt_mask.\"\n",
    "assert expected_ntokens == output.ntokens.item(), \"Test Failed: Incorrect values in response for ntokens.\"\n",
    "print(\"collate_batch - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2254d47acdc415d",
   "metadata": {
    "collapsed": false,
    "id": "f2254d47acdc415d"
   },
   "source": [
    "## 2. Transformer Implementation [38 points - Programming]\n",
    "\n",
    "Transformers have revolutionized the field of natural language processing (NLP) and beyond, offering remarkable improvements in tasks like machine translation, text summarization, and sentiment analysis. Developed by [Vaswani et al. in their 2017 paper, \"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) , transformers replace traditional recurrent layers with self-attention mechanisms, enabling the model to process input data in parallel and capture complex dependencies across positions.\n",
    "\n",
    "This assignment is designed to deepen your understanding of transformers by guiding you through the process of implementing one from scratch. By building a transformer model piece by piece, you will gain insights into the architecture’s inner workings, including the attention mechanism, positional encoding, and the overall encoder-decoder structure.\n",
    "\n",
    "Most of the code related to this section will be implemented in `model.py`. You will be implementing the following classes and functions.\n",
    "\n",
    "The code break-up as per the famous diagram of transformer can be visualized as below:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/transformer_layers.png\" alt=\"Transformer\">\n",
    "</p>\n",
    "\n",
    "The code implementation will be in `model.py`. The tests for the implementation will be in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c8017bdd07cf31",
   "metadata": {
    "collapsed": false,
    "id": "d9c8017bdd07cf31"
   },
   "source": [
    "### 2.1. Positional Encoding [3 points - Programming]\n",
    "\n",
    "The PositionalEncoding module in PyTorch adds necessary positional dynamics to the input embeddings of transformers. Without these encodings, transformers would not be able to utilize the order of the sequence, as they lack any recurrence or convolutional structures inherent in their design.\n",
    "\n",
    "This module computes a unique encoding for each position in the sequence up to a specified max_len. The encodings use a combination of sine and cosine functions across different dimensions of the embeddings. Specifically, for each dimension i, the encoding alternates between a sine and cosine function:\n",
    "- Even indices (0, 2, 4, ...) use sine,\n",
    "- Odd indices (1, 3, 5, ...) use cosine.\n",
    "\n",
    "The equations are -\n",
    "$$ PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) $$\n",
    "$$ PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) $$\n",
    "\n",
    "The use of sinusoidal functions helps the model to easily learn to attend by relative positions without much computational overhead. The frequencies of the sine and cosine functions decrease with the dimension, providing a smooth gradient flow for positions that are closer in sequence and differing signals for positions that are further apart.\n",
    "\n",
    "After computing the positional encodings, they are added directly to the input embeddings. A dropout layer is then applied to the result to reduce overfitting and improve the generalization capabilities of the model during training. The use of dropout is a common regularization technique in neural network training.\n",
    "\n",
    "Implement the PositionalEncoding class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ff5f8ad4c99890",
   "metadata": {
    "id": "c1ff5f8ad4c99890"
   },
   "outputs": [],
   "source": [
    "from model import PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d8e442ee5595d",
   "metadata": {
    "id": "611d8e442ee5595d"
   },
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(torch.zeros(1, 100, 20))\n",
    "\n",
    "\n",
    "## Visualize your Positional Encodings (feel free to change to other dimensions)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.arange(100), y[0, :, 6].numpy(), label='Dim 6')\n",
    "plt.plot(np.arange(100), y[0, :, 8].numpy(), label='Dim 8')\n",
    "plt.plot(np.arange(100), y[0, :, 2].numpy(), label='Dim 2')\n",
    "plt.plot(np.arange(100), y[0, :, 5].numpy(), label='Dim 5')\n",
    "plt.legend()\n",
    "plt.title('Positional Encoding')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497fa4d1d8f4bf04",
   "metadata": {
    "id": "497fa4d1d8f4bf04"
   },
   "outputs": [],
   "source": [
    "## LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "pe = PositionalEncoding(4, 0)\n",
    "y = pe.forward(torch.zeros(1, 5, 4))\n",
    "expected_output = torch.tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
    "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
    "         [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
    "         [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
    "         [-0.7568, -0.6536,  0.0400,  0.9992]]])\n",
    "assert y.shape == (1, 5, 4), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(y, expected_output, atol=1e-4), \"Test Failed: Incorrect values in response for positional encoding without dropout.\"\n",
    "\n",
    "pe = PositionalEncoding(2, 0.1)\n",
    "x = torch.tensor([[[1, 2], [4, 5]]], dtype=torch.float32)\n",
    "y = pe.forward(x)\n",
    "expected_output = torch.tensor([[[1.1111, 3.3333],\n",
    "         [5.3794, 6.1559]]]\n",
    ")\n",
    "assert y.shape == (1, 2, 2), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(y, expected_output, atol=1e-4), \"Test Failed: Incorrect values in response for positional encoding with dropout.\"\n",
    "\n",
    "print(\"Positional Encoding - All tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f4a5d7c9e4225",
   "metadata": {
    "collapsed": false,
    "id": "f58f4a5d7c9e4225"
   },
   "source": [
    "### 2.2. Embeddings [4 points - Programming]\n",
    "\n",
    "The Embeddings provide functionality to transform token indices into dense embeddings. It also scales these embeddings by the square root of the embedding dimension. This scaling is crucial as it helps in maintaining a consistent variance across the embeddings, which is beneficial for stabilizing gradients during training.\n",
    "\n",
    "This is the same embedding as HW1, with slight change (as you will see in the docstring of the class).\n",
    "\n",
    "Implement the Embeddings class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa014f661916a2",
   "metadata": {
    "id": "43aa014f661916a2"
   },
   "outputs": [],
   "source": [
    "## LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from model import Embeddings\n",
    "\n",
    "emb = Embeddings(5, 5)\n",
    "emb.set_embedding_weights()\n",
    "x = torch.tensor([[1, 2, 3, 4, 0], [4, 3, 2, 1, 0]])\n",
    "y = emb(x)\n",
    "expected_output = torch.tensor([[[0.0000, 0.5590, 1.1180, 1.6771, 2.2361],\n",
    "         [0.0000, 0.5590, 1.1180, 1.6771, 2.2361],\n",
    "         [0.0000, 0.5590, 1.1180, 1.6771, 2.2361],\n",
    "         [0.0000, 0.5590, 1.1180, 1.6771, 2.2361],\n",
    "         [0.0000, 0.5590, 1.1180, 1.6771, 2.2361]],\n",
    "\n",
    "        [[0.0000, 0.5590, 1.1180, 1.6771, 2.2361],\n",
    "         [0.0000, 0.5590, 1.1180, 1.6771, 2.2361],\n",
    "         [0.0000, 0.5590, 1.1180, 1.6771, 2.2361],\n",
    "         [0.0000, 0.5590, 1.1180, 1.6771, 2.2361],\n",
    "         [0.0000, 0.5590, 1.1180, 1.6771, 2.2361]]])\n",
    "\n",
    "assert y.shape == (2, 5, 5), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(y, expected_output, atol=1e-4), \"Test Failed: Incorrect values in response for embeddings.\"\n",
    "print(\"Embeddings - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af929d4057573cbd",
   "metadata": {
    "collapsed": false,
    "id": "af929d4057573cbd"
   },
   "source": [
    "### 2.3. Scaled Dot-Product Attention [4 points - Programming]\n",
    "\n",
    "The attention function is an implementation of the Scaled Dot Product Attention mechanism, which is crucial for models that include transformers. This function calculates attention weights and applies these weights to the values based on the input queries and keys. The attention mechanism allows the model to focus on different parts of the input sequence, enabling it to capture complex dependencies and relationships between tokens.\n",
    "\n",
    "The Scaled Dot Product Attention function is defined as follows:\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "Here, Q, K, and V are the query, key, and value matrices, respectively. The function first computes the dot product of the query and key matrices, scales the result by the square root of the key dimension, and then applies a softmax function to obtain the attention weights. Finally, the function multiplies the attention weights by the value matrix to produce the output.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/scaled_dot_product_attention.png\" alt=\"Scaled Dot-product Attention\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "Implement the `attention()` function in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413ff71d53ede80",
   "metadata": {
    "id": "7413ff71d53ede80"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from model import attention\n",
    "\n",
    "q = torch.tensor([[[[1, 2, 3], [2, 1, 2], [3, 2, 1]],[[1, 3, 2], [2, 2, 1], [1, 2, 3]]]], dtype=torch.float32)\n",
    "k = torch.tensor([[[[1, 3, 1], [4, 1, 2], [3, 2, 1]], [[3, 1, 2], [1, 4, 2], [4, 2, 1]]]], dtype=torch.float32)\n",
    "v = torch.tensor([[[[3, 1, 2], [1, 2, 1], [2, 1, 3]], [[1, 3, 2], [2, 1, 3], [3, 2, 1]]]], dtype=torch.float32)\n",
    "\n",
    "attended_values, attention_weights = attention(q, k, v)\n",
    "expected_attended_values_1 = torch.tensor([[[[1.5799, 1.6134, 1.5799],\n",
    "          [1.1982, 1.8277, 1.3188],\n",
    "          [1.2806, 1.7427, 1.4914]],\n",
    "         [[2.0356, 1.0847, 2.8797],\n",
    "          [2.4735, 1.7788, 1.7477],\n",
    "          [2.0000, 1.2486, 2.7514]]]])\n",
    "expected_attention_weights_1 = torch.tensor([[[[0.1933, 0.6134, 0.1933],\n",
    "          [0.0259, 0.8277, 0.1464],\n",
    "          [0.0232, 0.7427, 0.2341]],\n",
    "         [[0.0164, 0.9317, 0.0519],\n",
    "          [0.1018, 0.3229, 0.5753],\n",
    "          [0.0829, 0.8343, 0.0829]]]])\n",
    "assert torch.allclose(attended_values, expected_attended_values_1, atol=1e-4), \"Test Failed: Incorrect values in response for Test Case 1 (no dropout, no mask).\"\n",
    "assert torch.allclose(attention_weights, expected_attention_weights_1, atol=1e-4), \"Test Failed: Incorrect values in response for Test Case 1 (no dropout, no mask).\"\n",
    "# Evaluate the effect of mask\n",
    "mask = torch.tensor([[[[1, 1, 1],\n",
    "                       [1, 1, 0],\n",
    "                       [1, 0, 0]]]], dtype=torch.float32)\n",
    "attended_values, attention_weights = attention(q, k, v, mask)\n",
    "expected_attended_values_2 = torch.tensor([[[[1.5799, 1.6134, 1.5799],\n",
    "          [1.0607, 1.9696, 1.0304],\n",
    "          [3.0000, 1.0000, 2.0000]],\n",
    "         [[2.0356, 1.0847, 2.8797],\n",
    "          [1.7604, 1.4793, 2.7604],\n",
    "          [1.0000, 3.0000, 2.0000]]]])\n",
    "expected_attention_weights_2 = torch.tensor([[[[0.1933, 0.6134, 0.1933],\n",
    "          [0.0304, 0.9696, 0.0000],\n",
    "          [1.0000, 0.0000, 0.0000]],\n",
    "         [[0.0164, 0.9317, 0.0519],\n",
    "          [0.2396, 0.7604, 0.0000],\n",
    "          [1.0000, 0.0000, 0.0000]]]])\n",
    "assert torch.allclose(attended_values, expected_attended_values_2, atol=1e-4), \"Test Failed: Incorrect values in response for Test Case 2 (no dropout but mask present).\"\n",
    "assert torch.allclose(attention_weights, expected_attention_weights_2, atol=1e-4), \"Test Failed: Incorrect values in response for Test Case 2 (no dropout but mask present).\"\n",
    "# Evaluate the effect of dropout\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "attended_values_1, attention_weights_1 = attention(q, k, v, dropout=dropout, mask=mask)\n",
    "attended_values_2, attention_weights_2 = attention(q, k, v, dropout=dropout, mask=mask)\n",
    "assert not torch.allclose(attention_weights_1, attention_weights_2, atol=1e-4), \"Test Failed: Incorrect results with dropout.\"\n",
    "assert not torch.allclose(attended_values_1, attended_values_2, atol=1e-4), \"Test Failed: Incorrect results with dropout.\"\n",
    "print(\"Scaled Dot-Product Attention - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec9ccab78ecfa7",
   "metadata": {
    "collapsed": false,
    "id": "21ec9ccab78ecfa7"
   },
   "source": [
    "### 2.4. Multi-Head Attention [4 points - Programming]\n",
    "\n",
    "The MultiHeadedAttention class encapsulates the multi-head attention mechanism, a pivotal component in transformer architectures. This mechanism allows the model to attend to different parts of the input sequence simultaneously, making it highly effective for tasks involving complex dependencies.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "- Parallel Attention Heads: Each head independently computes attention, focusing on different features of the input. The results from each head are then combined to capture a richer representation.\n",
    "- Dimensionality: The input dimensions are projected into smaller, separate dimensions for each head $(d_k)$, speeding up computation and allowing diverse representations.\n",
    "- Scalability: By separating the input into multiple heads, the model can scale attention across different subspaces, enhancing learning capabilities and model performance.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/multihead_attention.png\" alt=\"Multihead Attention\">\n",
    "</p>\n",
    "\n",
    "In `model.py`, implement the `MultiHeadedAttention` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cedc6835dc278f",
   "metadata": {
    "id": "46cedc6835dc278f"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from model import MultiHeadedAttention\n",
    "q = torch.tensor([[[1, 2, 3], [2, 1, 2], [3, 2, 1]], [[1, 3, 2], [2, 2, 1], [1, 2, 3]]], dtype=torch.float32)\n",
    "k = torch.tensor([[[1, 3, 1], [4, 1, 2], [3, 2, 1]], [[3, 1, 2], [1, 4, 2], [4, 2, 1]]], dtype=torch.float32)\n",
    "v = torch.tensor([[[3, 1, 2], [1, 2, 1], [2, 1, 3]], [[1, 3, 2], [2, 4, 3], [3, 2, 1]]], dtype=torch.float32)\n",
    "mha = MultiHeadedAttention(3, 3, dropout=0.0)\n",
    "weights = torch.tensor([[[1, 2, 1], [1, 1, 2], [2, 1, 1]], [[1, 3, 1], [1, 3, 2], [2, 4, 1]], [[1, 2, 3], [1, 4, 2], [2, 3, 1]], [[1, 4, 3], [1, 1, 4], [4, 3, 1]]], dtype=torch.float32)\n",
    "biases = torch.tensor([[1, 2, 4], [1, 3, 5], [2, 5, 1], [4, 1, 3]], dtype=torch.float32)\n",
    "mha.set_weights(weights, biases)\n",
    "out = mha(q, k, v)\n",
    "expected_output = torch.tensor([[[115.5003,  76.0002, 114.5010],\n",
    "         [115.5023,  76.0019, 114.5076],\n",
    "         [115.5007,  76.0004, 114.5014]],\n",
    "        [[201.0000, 131.0000, 194.0000],\n",
    "         [201.0000, 131.0000, 194.0000],\n",
    "         [201.0000, 131.0000, 194.0000]]])\n",
    "assert out.shape == (2, 3, 3), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(out, expected_output, atol=1e-4), \"Test Failed: Incorrect results without dropout.\"\n",
    "mha = MultiHeadedAttention(3, 3, dropout=0.5)\n",
    "out_1 = mha(q, k, v)\n",
    "out_2 = mha(q, k, v)\n",
    "assert not torch.allclose(out_1, out_2, atol=1e-4), \"Test Failed: Incorrect results with dropout.\"\n",
    "print(\"MultiHeadedAttention - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa44ae26c424cf2",
   "metadata": {
    "collapsed": false,
    "id": "cfa44ae26c424cf2"
   },
   "source": [
    "### 2.5. Feed-Forward Layer [3 points - Programming]\n",
    "\n",
    "In both the encoder and decoder of our model, each layer includes a fully connected feed-forward network. This network operates independently on each position and follows the same structure throughout. It is composed of two linear transformations. Between these transformations, a ReLU activation function is applied to introduce non-linearity.\n",
    "\n",
    "$$ \\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2 $$\n",
    "\n",
    "Implement the `FeedForward` class in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec63cfc9f8f8b8d",
   "metadata": {
    "id": "5ec63cfc9f8f8b8d"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from model import FeedForward\n",
    "x = torch.tensor([[[1, 2, 3], [2, 1, 2], [3, 2, 1]], [[1, 3, 2], [2, 2, 1], [1, 2, 3]]], dtype=torch.float32)\n",
    "ffn = FeedForward(3, 3, 0.0)\n",
    "weights = torch.tensor([[[1, 2, 1], [1, 1, 2], [2, 1, 1]], [[1, 3, 1], [4, 3, 2], [2, 2, 1]]], dtype=torch.float32)\n",
    "biases = torch.tensor([[1, 2, 4], [2, 2, 3]], dtype=torch.float32)\n",
    "ffn.set_weights(weights, biases)\n",
    "out = ffn(x)\n",
    "expected_output = torch.tensor([[[55., 93., 54.],\n",
    "         [47., 79., 46.],\n",
    "         [51., 91., 52.]],\n",
    "        [[53., 94., 54.],\n",
    "         [45., 80., 46.],\n",
    "         [55., 93., 54.]]])\n",
    "assert out.shape == (2, 3, 3), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(out, expected_output, atol=1e-4), \"Test Failed: Incorrect results without dropout.\"\n",
    "ffn = FeedForward(3, 3, 0.5)\n",
    "out_1 = ffn(x)\n",
    "out_2 = ffn(x)\n",
    "assert not torch.allclose(out_1, out_2, atol=1e-4), \"Test Failed: Incorrect results with dropout.\"\n",
    "print(\"FeedForward - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d5c088403ce74c",
   "metadata": {
    "collapsed": false,
    "id": "b0d5c088403ce74c"
   },
   "source": [
    "### 2.6. Layer Normalization [3 points - Programming]\n",
    "\n",
    "Layer normalization is a technique that normalizes the inputs across the features instead of the batch. It is very effective for stabilizing the learning process in networks that are deep or use complex recurrent structures. The normalization process adjusts the input data in such a way that the mean output per feature vector over a training batch is zero and the standard deviation is one. This is particularly beneficial in models like transformers where the batch size can often vary.\n",
    "\n",
    "Layer normalization differs from batch normalization as it normalizes across the features for each single sample, and not across the batch dimension. It is extensively used in transformer models.\n",
    "\n",
    "You can read more about Layer Normalization [here](https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "Equation -\n",
    "$$ y = \\alpha \\odot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta $$\n",
    "\n",
    "Where:\n",
    "- x is the input tensor.\n",
    "- μ is the mean computed over the last dimension.\n",
    "- σ is the standard deviation computed over the last dimension.\n",
    "- α is a learnable parameter scaling each feature.\n",
    "- β is a learnable parameter shifting each feature.\n",
    "- ϵ is a small constant (epsilon) added for numerical stability.\n",
    "\n",
    "Implement the `LayerNorm` class in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66146eae60532ce4",
   "metadata": {
    "id": "66146eae60532ce4"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from model import LayerNorm\n",
    "\n",
    "x = torch.tensor([[[1, 2, 3], [2, 1, 2], [3, 2, 1]], [[1, 3, 2], [2, 2, 1], [1, 2, 3]]], dtype=torch.float32)\n",
    "ln = LayerNorm(3)\n",
    "out = ln(x)\n",
    "expected_output = torch.tensor([[[-1.0000,  0.0000,  1.0000],\n",
    "         [ 0.5773, -1.1547,  0.5773],\n",
    "         [ 1.0000,  0.0000, -1.0000]],\n",
    "        [[-1.0000,  1.0000,  0.0000],\n",
    "         [ 0.5773,  0.5773, -1.1547],\n",
    "         [-1.0000,  0.0000,  1.0000]]])\n",
    "assert out.shape == (2, 3, 3), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(out, expected_output, atol=1e-4), \"Test Failed: Incorrect results.\"\n",
    "print(\"LayerNorm - All tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f9ac20ace8672",
   "metadata": {
    "collapsed": false,
    "id": "af7f9ac20ace8672"
   },
   "source": [
    "### 2.7. Residual Stream Block [3 points - Programming]\n",
    "\n",
    "The Residual Stream Block is designed to encapsulate two primary functionalities essential in modern neural network architectures: normalization and residual connections. This combination is particularly popular in transformer models and helps combat the vanishing gradient problem in deep networks.\n",
    "\n",
    "- Layer Normalization: This step normalizes the input tensor's features to have zero mean and unit variance, which stabilizes learning by making the model less sensitive to different input scales.\n",
    "- Residual Connection: The residual connection allows gradients to flow through the network directly by adding the input tensor x to the output of a transformation applied to x. This skip-connection technique is critical for training deeper networks efficiently.\n",
    "- Dropout: Integrated into the pathway, dropout is a regularization technique that \"drops out\" random neuron outputs during training to prevent overfitting and encourage robust internal representations.\n",
    "\n",
    "Implement the `ResidualStreamBlock` class in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9db3689619d426",
   "metadata": {
    "id": "6b9db3689619d426"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from model import ResidualStreamBlock\n",
    "x = torch.tensor([[[1, 2, 3], [2, 1, 2], [3, 2, 1]], [[1, 3, 2], [2, 2, 1], [1, 2, 3]]], dtype=torch.float32)\n",
    "linear = torch.nn.Linear(3, 3, bias=False)\n",
    "linear.weight.data = torch.tensor([[1, 2, 1], [1, 1, 2], [2, 1, 1]], dtype=torch.float32)\n",
    "scl = ResidualStreamBlock(3, 0.0)\n",
    "out = scl(x, linear)\n",
    "expected_output = torch.tensor([[[1.0000, 3.0000, 2.0000],\n",
    "         [0.8453, 1.5773, 2.5773],\n",
    "         [3.0000, 1.0000, 2.0000]],\n",
    "        [[2.0000, 3.0000, 1.0000],\n",
    "         [2.5773, 0.8453, 1.5773],\n",
    "         [1.0000, 3.0000, 2.0000]]])\n",
    "assert out.shape == (2, 3, 3), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(out, expected_output, atol=1e-4), \"Test Failed: Incorrect results without dropout.\"\n",
    "scl = ResidualStreamBlock(3, 0.5)\n",
    "out_1 = scl(x, linear)\n",
    "out_2 = scl(x, linear)\n",
    "assert not torch.allclose(out_1, out_2, atol=1e-4), \"Test Failed: Incorrect results with dropout.\"\n",
    "print(\"ResidualStreamBlock - All tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5dafe18bd0e52e",
   "metadata": {
    "collapsed": false,
    "id": "db5dafe18bd0e52e"
   },
   "source": [
    "### 2.8. Autoregressive Masking [3 points - Programming]\n",
    "\n",
    "An autoregressive mask generates a boolean mask used to control the flow of information in the sequence. This mask that output at a certain position in a sequence is only dependent on the previous positions and not any future positions. This mechanism is crucial in tasks like sequence generation where the model should not have access to future input during training.\n",
    "\n",
    "Implement the `autoregressive_mask` function in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4304907a1e902329",
   "metadata": {
    "id": "4304907a1e902329"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from model import autoregressive_mask\n",
    "mask = autoregressive_mask(3)\n",
    "expected_output = torch.tensor([[[1, 0, 0],\n",
    "         [1, 1, 0],\n",
    "         [1, 1, 1]]], dtype=torch.bool)\n",
    "assert mask.shape == (1, 3, 3), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(mask, expected_output, atol=1e-4), \"Test Failed: Incorrect results.\"\n",
    "mask = autoregressive_mask(7)\n",
    "expected_output = torch.tensor([[[1, 0, 0, 0, 0, 0, 0],\n",
    "         [1, 1, 0, 0, 0, 0, 0],\n",
    "         [1, 1, 1, 0, 0, 0, 0],\n",
    "         [1, 1, 1, 1, 0, 0, 0],\n",
    "         [1, 1, 1, 1, 1, 0, 0],\n",
    "         [1, 1, 1, 1, 1, 1, 0],\n",
    "         [1, 1, 1, 1, 1, 1, 1]]], dtype=torch.bool)\n",
    "assert mask.shape == (1, 7, 7), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(mask, expected_output, atol=1e-4), \"Test Failed: Incorrect results.\"\n",
    "print(\"Subsequent Mask - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac38d2ce990350",
   "metadata": {
    "collapsed": false,
    "id": "76ac38d2ce990350"
   },
   "source": [
    "These are the basic building blocks of the Transformer model. In the next sections, we will implement the Encoder and Decoder classes, and then combine them to create the full Transformer model.\n",
    "Note, the encoder-decoder transformer is the fundamental form of transformer with self-attention and cross-attention. It is widely used in machine translation tasks, or summarization tasks.\n",
    "\n",
    "Apart from there, there are also encoder-only transformers, and decoder-only transformers. The encoder-only transformers are used in tasks like language modeling, where the model is trained to predict the next word in a sequence. The decoder-only transformers are used in tasks like text generation, where the model is trained to generate a sequence of words given an input sequence.\n",
    "\n",
    "BERT is an example of an encoder-only transformer, while GPT is an example of a decoder-only transformer. In this assignment, we will be implementing the encoder-decoder transformer. Some encoder-decoder transformers are BART, T5, etc.\n",
    "\n",
    "Let's move to utilizing the building blocks we have implemented so far to create the Encoder and Decoder classes, and then combine them to create the full Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9684b9551eba787f",
   "metadata": {
    "collapsed": false,
    "id": "9684b9551eba787f"
   },
   "source": [
    "### 2.9. Transformer Encoder Block [3 points - Programming]\n",
    "\n",
    "It is designed to encapsulate two main submodules: self-attention and a feed-forward network.\n",
    "\n",
    "- Self-attention Module: This module allows each position in the encoder to attend to all positions in the previous layer of the encoder, which captures intricate dependencies in the input data regardless of their distance in the input sequence. This is an instance of multi-head attention, where the query, key, and value inputs are all derived from the same input.\n",
    "- Feed-forward Network: Each position is separately and identically processed by the feed-forward network, which typically consists of two linear transformations with a ReLU activation in between. This is the same feed-forward network we implemented earlier.\n",
    "- Sublayer Connections: Each of these components is embedded in a ResidualStreamBlock, which applies a residual connection followed by layer normalization. This design promotes direct pathways for gradients during training, facilitating the training of deep networks. This is the same sublayer connection we implemented earlier.\n",
    "\n",
    "Process Flow in forward Method:\n",
    "- The input tensor x is first passed through the self-attention mechanism within a sublayer connection, which also uses a mask to ignore certain positions (useful for handling padded positions in sequences).\n",
    "- The output of the self-attention is then passed through the feed-forward network within another sublayer connection.\n",
    "- This structure effectively allows the encoder to enhance its representation capabilities with depth, leveraging both self-attention and dense layers, while maintaining stability through normalization and residual connections.\n",
    "\n",
    "Implement the `EncoderBlock` class in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949679507b65bd81",
   "metadata": {
    "id": "949679507b65bd81"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from model import EncoderBlock\n",
    "from model import MultiHeadedAttention, FeedForward\n",
    "x = torch.tensor([[[1, 2, 3], [2, 1, 2], [3, 2, 1]], [[1, 3, 2], [2, 2, 1], [1, 2, 3]]], dtype=torch.float32)\n",
    "mha = MultiHeadedAttention(3, 3, dropout=0.0)\n",
    "weights = torch.tensor([[[1, 2, 1], [1, 1, 2], [2, 1, 1]], [[1, 3, 1], [1, 3, 2], [2, 4, 1]], [[1, 2, 3], [1, 4, 2], [2, 3, 1]], [[1, 4, 3], [1, 1, 4], [4, 3, 1]]], dtype=torch.float32)\n",
    "biases = torch.tensor([[1, 2, 4], [1, 3, 5], [2, 5, 1], [4, 1, 3]], dtype=torch.float32)\n",
    "mha.set_weights(weights, biases)\n",
    "ffn = FeedForward(3, 3, 0.0)\n",
    "weights = torch.tensor([[[1, 2, 1], [1, 1, 2], [2, 1, 1]], [[1, 3, 1], [4, 3, 2], [2, 2, 1]]], dtype=torch.float32)\n",
    "biases = torch.tensor([[1, 2, 4], [2, 2, 3]], dtype=torch.float32)\n",
    "ffn.set_weights(weights, biases)\n",
    "eb = EncoderBlock(3, mha, ffn, 0.0)\n",
    "mask = None\n",
    "out = eb(x, mask)\n",
    "expected_output = torch.tensor([[[50.9330, 37.6713, 46.5334],\n",
    "         [51.7019, 36.5467, 45.3967],\n",
    "         [50.7294, 36.9257, 42.9702]],\n",
    "\n",
    "        [[59.8909, 43.0178, 53.5605],\n",
    "         [58.5936, 41.4671, 50.4271],\n",
    "         [60.3661, 41.9595, 54.0644]]])\n",
    "\n",
    "assert out.shape == (2, 3, 3), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(out, expected_output, atol=1e-4), \"Test Failed: Incorrect results without dropout.\"\n",
    "eb = EncoderBlock(3, mha, ffn, 0.5)\n",
    "out_1 = eb(x, mask)\n",
    "out_2 = eb(x, mask)\n",
    "assert not torch.allclose(out_1, out_2, atol=1e-4), \"Test Failed: Incorrect results with dropout.\"\n",
    "print(\"EncoderBlock - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501d5e9300b55eb",
   "metadata": {
    "collapsed": false,
    "id": "2501d5e9300b55eb"
   },
   "source": [
    "### 2.10. Transformer Encoder\n",
    "\n",
    "The encoder consists of a stack of identical layers, each of which may involve operations such as self-attention and feed-forward transformations. Each layer is expected to maintain a consistent interface and dimensionality, specified by the size attribute. After data passes through all the layers, a final layer normalization is applied. This step is crucial as it ensures that the outputs of the encoder are normalized, reducing the risk of instability in gradients and aiding in faster convergence.\n",
    "\n",
    "The forward method illustrates the sequential processing of the input $x$ through each layer in the stack, with an optional mask applied at each step. The mask is typically used to nullify the effects of padding in sequence data, ensuring that padding does not influence the learning process. By allowing the encoder to stack an arbitrary number of identical layers, this design supports scalability and depth in model architecture, enabling it to learn more complex features and relationships in data.\n",
    "\n",
    "This implementation is streamlined yet flexible, accommodating any layer structure that fits the defined interface, making it a versatile component in advanced neural network architectures.\n",
    "\n",
    "Implement the `Encoder` class in `model.py`.\n",
    "There are no local tests for this cell due to large complexity in initializations. The implementation only requires stacking the EncoderBlock, and reusing existing classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9e6e7572fe996",
   "metadata": {
    "id": "dbf9e6e7572fe996"
   },
   "outputs": [],
   "source": [
    "# EXECUTION CHECK\n",
    "from model import Encoder, EncoderBlock, MultiHeadedAttention, FeedForward\n",
    "x = torch.tensor([[[1, 2, 3], [2, 1, 2], [3, 2, 1]], [[1, 3, 2], [2, 2, 1], [1, 2, 3]]], dtype=torch.float32)\n",
    "encoder_block = EncoderBlock(3, MultiHeadedAttention(3, 3, dropout=0.0), FeedForward(3, 3, 0.0), 0.0)\n",
    "encoder = Encoder(encoder_block, 2)\n",
    "\n",
    "print(encoder)\n",
    "\n",
    "# Try to map this to the transformers architecture diagram and understand if you have got the entire encoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886a0a31fed8b85",
   "metadata": {
    "collapsed": false,
    "id": "3886a0a31fed8b85"
   },
   "source": [
    "### 2.11. Transformer Decoder Block [3 points - Programming]\n",
    "\n",
    "The Decoder block in transformer models is designed to process the output from the encoder and generate a sequence output in tasks like translation or text generation. Each decoder layer contains three key sub-modules:\n",
    "- a self-attention mechanism, which helps the decoder focus on relevant parts of the input sequence\n",
    "- a cross-attention mechanism, allowing the decoder to attend to the entire output of the encoder\n",
    "- a position-wise feed-forward network, which applies a set of fully connected layers to each position independently.\n",
    "\n",
    "These sub-modules are encapsulated within a framework that includes residual connections and layer normalization, crucial for stabilizing the learning process and enabling deeper model architectures. Understanding these components is essential for students delving into how transformers maintain context and generate coherent outputs in sequence-to-sequence tasks.\n",
    "\n",
    "Implement the `DecoderBlock` class in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fa36ddecfa713",
   "metadata": {
    "id": "297fa36ddecfa713"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from model import DecoderBlock, MultiHeadedAttention, FeedForward, ResidualStreamBlock, autoregressive_mask\n",
    "x = torch.tensor([[[1, 2, 3], [2, 1, 2], [3, 2, 1]], [[1, 3, 2], [2, 2, 1], [1, 2, 3]]], dtype=torch.float32)\n",
    "mha_self = MultiHeadedAttention(3, 3, dropout=0.0)\n",
    "weights = torch.tensor([[[1, 2, 1], [1, 1, 2], [2, 1, 1]], [[1, 3, 1], [1, 3, 2], [2, 4, 1]], [[1, 2, 3], [1, 4, 2], [2, 3, 1]], [[1, 4, 3], [1, 1, 4], [4, 3, 1]]], dtype=torch.float32)\n",
    "biases = torch.tensor([[1, 2, 4], [1, 3, 5], [2, 5, 1], [4, 1, 3]], dtype=torch.float32)\n",
    "mha_self.set_weights(weights, biases)\n",
    "mha_cross = MultiHeadedAttention(3, 3, dropout=0.0)\n",
    "weights = torch.tensor([[[1, 2, 1], [1, 1, 2], [2, 9, 1]], [[1, 3, 1], [4, 3, 2], [2, 2, 1]], [[5, 2, 1], [1, 3, 2], [2, 1, 1]], [[1, 3, 0], [4, 3, 2], [2, 2, 6]]], dtype=torch.float32)\n",
    "biases = torch.tensor([[1, 2, 2], [1, 3, 7], [2, 5, 4], [4, 9, 3]], dtype=torch.float32)\n",
    "mha_cross.set_weights(weights, biases)\n",
    "ffn = FeedForward(3, 3, 0.0)\n",
    "weights = torch.tensor([[[1, 2, 1], [1, 1, 2], [2, 1, 1]], [[1, 3, 1], [4, 3, 2], [2, 2, 1]]], dtype=torch.float32)\n",
    "biases = torch.tensor([[1, 2, 4], [2, 2, 3]], dtype=torch.float32)\n",
    "ffn.set_weights(weights, biases)\n",
    "db = DecoderBlock(3, mha_self, mha_cross, ffn, 0.0)\n",
    "src_mask = None\n",
    "tgt_mask = autoregressive_mask(3)\n",
    "memory = torch.tensor([[[1, 1, 0], [2, 3, 1], [1, 2, 1]], [[1, 1, 1], [3, 2, 1], [1, 2, 4]]], dtype=torch.float32)\n",
    "out = db(x, memory, src_mask, tgt_mask)\n",
    "\n",
    "expected_output = torch.tensor([[[117.3262, 158.9924, 158.8624],\n",
    "         [116.6737, 155.3791, 152.4066],\n",
    "         [121.2165, 166.5353, 150.3511]],\n",
    "        [[126.4522, 174.3401, 168.4910],\n",
    "         [127.5165, 177.1474, 165.8177],\n",
    "         [127.0625, 175.4266, 167.4181]]])\n",
    "\n",
    "assert out.shape == (2, 3, 3), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(out, expected_output, atol=1e-4), \"Test Failed: Incorrect results without dropout.\"\n",
    "db = DecoderBlock(3, mha_self, mha_cross, ffn, 0.5)\n",
    "out_1 = db(x, memory, src_mask, tgt_mask)\n",
    "out_2 = db(x, memory, src_mask, tgt_mask)\n",
    "assert not torch.allclose(out_1, out_2, atol=1e-4), \"Test Failed: Incorrect results with dropout.\"\n",
    "print(\"DecoderBlock - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f783994bde20da",
   "metadata": {
    "collapsed": false,
    "id": "d0f783994bde20da"
   },
   "source": [
    "### 2.12. Transformer Decoder\n",
    "\n",
    "The decoder consists of a stack of identical decoder blocks. Each layer is expected to maintain a consistent interface and dimensionality, specified by the size attribute. After data passes through all the layers, a final layer normalization is applied.\n",
    "\n",
    "Decoder is constructed with DecoderBlock just the same way as Encoder is constructed with EncoderBlock.\n",
    "\n",
    "Implement the `Decoder` class in `model.py`.\n",
    "\n",
    "There are no local tests for this cell due to large complexity in initializations. The implementation only requires stacking the `DecoderBlock`, and reusing existing classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cac62f2628d234",
   "metadata": {
    "id": "c1cac62f2628d234"
   },
   "outputs": [],
   "source": [
    "# EXECUTION CHECK\n",
    "from model import Decoder, DecoderBlock, MultiHeadedAttention, FeedForward, ResidualStreamBlock, autoregressive_mask\n",
    "x = torch.tensor([[[1, 2, 3], [2, 1, 2], [3, 2, 1]], [[1, 3, 2], [2, 2, 1], [1, 2, 3]]], dtype=torch.float32)\n",
    "decoder_block = DecoderBlock(3, MultiHeadedAttention(3, 3, dropout=0.0), MultiHeadedAttention(3, 3, dropout=0.0), FeedForward(3, 3, 0.0), 0.0)\n",
    "decoder = Decoder(decoder_block, 2)\n",
    "\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8833681c7501eb0c",
   "metadata": {
    "collapsed": false,
    "id": "8833681c7501eb0c"
   },
   "source": [
    "### 2.13. Generator [3 points - Programming]\n",
    "\n",
    "The Generator class serves as the final component of the decoder output pipeline, transforming high-dimensional decoder outputs into a probability distribution over a predefined vocabulary. It achieves this by employing a linear transformation that maps the decoder's feature space to the vocabulary space, followed by a log softmax function. The log softmax ensures the output values are log probabilities, which are more numerically stable for the computation of loss functions like cross-entropy during training. Understanding the mechanics of the Generator is crucial for those looking to delve into the specifics of neural output generation, particularly how models like Transformers predict the next sequence of tokens in a given task.\n",
    "\n",
    "Implement the `Generator` class in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a861869bdba1becf",
   "metadata": {
    "id": "a861869bdba1becf"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "import model\n",
    "\n",
    "from model import Generator\n",
    "\n",
    "generator = Generator(3, 10)\n",
    "weight = torch.tensor([[1, 2, 1], [1, 1, 2], [2, 1, 1], [1, 3, 1], [1, 3, 2], [2, 4, 1], [5, 2, 1], [1, 3, 2], [2, 1, 1], [1, 3, 0]], dtype=torch.float32)\n",
    "bias = torch.tensor([1, 2, 4, 2, 2, 3, 2, 5, 4, 9], dtype=torch.float32)\n",
    "\n",
    "x = torch.tensor([[[1, 2, 3], [2, 1, 2], [3, 2, 1]], [[1, 3, 2], [2, 2, 1], [1, 2, 3]]], dtype=torch.float32)\n",
    "\n",
    "generator.set_weights(weight, bias)\n",
    "out = generator(x)\n",
    "expected_output = torch.tensor([[[ -9.2957,  -7.2957,  -7.2957,  -6.2957,  -3.2957,  -2.2957,  -4.2957,\n",
    "           -0.2957,  -7.2957,  -2.2957],\n",
    "         [ -9.2946,  -7.2946,  -5.2946,  -7.2946,  -5.2946,  -3.2946,  -0.2946,\n",
    "           -2.2946,  -5.2946,  -2.2946],\n",
    "         [-13.0388, -13.0388,  -9.0388, -10.0388,  -9.0388,  -4.0388,  -0.0388,\n",
    "           -6.0388,  -9.0388,  -4.0388]],\n",
    "        [[-10.1236, -10.1236,  -9.1236,  -6.1236,  -4.1236,  -1.1236,  -5.1236,\n",
    "           -1.1236,  -9.1236,  -1.1236],\n",
    "         [ -9.9233,  -9.9233,  -6.9233,  -6.9233,  -5.9233,  -1.9233,  -0.9233,\n",
    "           -2.9233,  -6.9233,  -0.9233],\n",
    "         [ -9.2957,  -7.2957,  -7.2957,  -6.2957,  -3.2957,  -2.2957,  -4.2957,\n",
    "           -0.2957,  -7.2957,  -2.2957]]])\n",
    "\n",
    "assert out.shape == (2, 3, 10), \"Test Failed: Incorrect shape of obtained response.\"\n",
    "assert torch.allclose(out, expected_output, atol=1e-4), \"Test Failed: Incorrect results.\"\n",
    "print(\"Generator - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65befca3b84b2f41",
   "metadata": {
    "collapsed": false,
    "id": "65befca3b84b2f41"
   },
   "source": [
    "### 2.14. The Complete Transformer\n",
    "\n",
    "Great, we have now implemented all components of the Transformer model. Before we move ahead, feel free to map your implementations to the diagram of the transformer. It will make a lot of things clear going forward.\n",
    "\n",
    "Let's now combine these components to create the full Transformer model. The Transformer model consists of an encoder, a decoder, and a generator. The encoder processes the input sequence, the decoder generates the output sequence, and the generator converts the decoder output into a probability distribution over the vocabulary.\n",
    "\n",
    "Implement the `Transformer` class in `model.py`.\n",
    "\n",
    "Like the Encoder and Decoder, this also won't have local tests due to the complexity in initializations.\n",
    "\n",
    "After this, to complete the model definition, thoroughly go through the `make_model()` function in `model.py`. This function will create an instance of the Transformer model with the specified hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f707affc17c4b",
   "metadata": {
    "collapsed": false,
    "id": "369f707affc17c4b"
   },
   "source": [
    "### 2.15. Inference Test on Untrained Transformer [2 points - Programming]\n",
    "\n",
    "Let's run a dummy inference on an untrained transformer to check if things are fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e7c1ec3b4eb32",
   "metadata": {
    "id": "5c5e7c1ec3b4eb32"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from model import make_model\n",
    "def inference_test():\n",
    "    test_model = make_model(11, 11, 2)\n",
    "    test_model.eval()\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).type_as(src)\n",
    "\n",
    "    for i in range(9):\n",
    "        out = test_model.decode(\n",
    "            memory, src_mask, ys, autoregressive_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "\n",
    "    print(\"Example Untrained Model Prediction:\", ys)\n",
    "\n",
    "\n",
    "def run_tests():\n",
    "    for _ in range(10):\n",
    "        inference_test()\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "        return fn(*args)\n",
    "\n",
    "show_example(run_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be35ab7",
   "metadata": {
    "id": "1be35ab7"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091694ba",
   "metadata": {
    "id": "091694ba"
   },
   "source": [
    "## 3. Model Training [5 points - Programming + 5 points - Non-programming]\n",
    "\n",
    "In this section, we will implement the training loop for the Transformer model. We will also implement the label smoothing technique to regularize the model during training.\n",
    "We have already provided most of the training utils that we asked you to implement during previous homeworks. You can find them in `utils.py`, `model_training.py` and `datautils.py`. Some of the implementations will be done by you in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878ee37",
   "metadata": {
    "id": "9878ee37"
   },
   "source": [
    "### 3.1 Label Smoothing [5 points - Programming]\n",
    "\n",
    "The label smoothing loss (introduced in [Szegedy 2015](https://arxiv.org/pdf/1512.00567)) is a regularization technique used to prevent the model from overfitting to the training data. The label smoothing loss is implemented in the `LabelSmoothing` class in the `utils.py` file.\n",
    "\n",
    "Let's say the model has a vocabulary $V$ so that the model outputs a $|V|$-dimensional vector each token. If `target` is the true label of the next token prediction task, then the *true distribution* is given by the $|V|$-dimensional vector $t$ where the `target` index is set to 1 and all other indices are set to 0.\n",
    "\n",
    "Since we want the model to be less confident, we would like to *smooth* this label by uniformly spreading the probability mass from the `target` index to all other indices. (In our implementation, we exclude the padding index from the smoothed distribution because we never want to predict the padding token.) Thus, the smoothed distribution vector $s$ is given by:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "s[\\mathrm{target}] = 1 - \\epsilon\n",
    "$$\n",
    "\n",
    "$$\n",
    "s[\\mathrm{pad\\_id}] = 0\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "s[i] = \\frac{\\epsilon}{|V|-1}      \n",
    "$$\n",
    "\n",
    "for all $i \\neq \\mathrm{target}, \\mathrm{pad\\_id}$.\n",
    "\n",
    "\n",
    "(For training, we will be feeding the LabelSmoothing loss through the `LossWrapper` class for convenience.)\n",
    "\n",
    "In addition, if `t = PAD_ID`, then we set `s` to be the zero vector. This is so that in our implementation, the padding tokens do not contribute to the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191dbc9a",
   "metadata": {
    "id": "191dbc9a"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from utils import LabelSmoothing\n",
    "import torch\n",
    "\n",
    "PAD_ID = 3\n",
    "VOCAB_SIZE = 10\n",
    "\n",
    "label_smoothing = LabelSmoothing(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    padding_idx=PAD_ID,\n",
    "    smoothing=0.1\n",
    ")\n",
    "out = torch.ones(8,VOCAB_SIZE).to(torch.float32)\n",
    "target = torch.ones(8).contiguous().view(-1).to(torch.int64)\n",
    "\n",
    "loss_val = label_smoothing(out, target).detach()\n",
    "expected_loss = torch.tensor(-11.86957)\n",
    "assert torch.isclose(loss_val, expected_loss, atol=1e-4), f\"Test Failed: Incorrect results. Expected {expected_loss}, but got {loss_val}\"\n",
    "print(\"Label Smoothing - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517c92f",
   "metadata": {
    "id": "2517c92f"
   },
   "source": [
    "### 3.2 Training Loop [5 points - Non-Programming]\n",
    "\n",
    "Since we have all the components ready, let's now implement the training loop.\n",
    "\n",
    "`REQUIREMENT`: To achieve full credits, submit the output of the two cells of Section 3.2.2., along with the training trace of last 3 epochs. It should show the losses, and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c379a3",
   "metadata": {
    "id": "d9c379a3"
   },
   "source": [
    "#### 3.2.1. Gradient Accumulation\n",
    "\n",
    "Gradient accumulation is a trick for training large models when you have limited GPU memory. Rather than updating the model weights after every batch (as in standard minibatch stochastic gradient descent), we only update the weights after passing through multiple batches. This means the gradients are *accumulated*, i.e. summed up over multiple batches.  We can thus simulate training with a large batch size while still using a GPU with a limited memory.\n",
    "\n",
    "Implement the `run_epoch()` function in `model_training.py`. (All the other functions are provided for you). We have implemented the training forward pass, you just need to implement the backward pass. We do not have local tests for this, so please follow the hints carefully. Gradient accumulation is important, please implement that correctly, else you may find your model hard to train.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7a033",
   "metadata": {
    "id": "52f7a033"
   },
   "source": [
    "#### 3.2.2. Model Training\n",
    "\n",
    "Run the following cell to train the model (or load a previously saved checkpoint). Note that training can take several hours.\n",
    "\n",
    "Go to `model_training.py` and adjust your hyperparameters in `ModelHyperParams` and `TrainingHyperParams`. We have already provided an initial set to give you an acceptable starting point, but feel free to make any changes.\n",
    "\n",
    "If your model fails, you can resume training from the last checkpoint by setting the `continue_training` parameter in the `TrainingHyperParams` dataclass to the path. Pass the path to the checkpoint to the `continue_training` parameter in the `TrainingHyperParams` dataclass. Make it `None` if you want to start training from scratch.\n",
    "\n",
    "Note, for getting acceptable results in the next section, it is recommended that your validation loss is less than 0.0002. However, this is not a strict requirement for this assignment. You may not achieve this and still succeed, or you may achieve this and still fail. The main goal is to implement the model correctly and understand the concepts.\n",
    "\n",
    "Some statistics: It took us 12 min/epoch and 20 GB GPU RAM on an Nvidia A100 GPU (on Google Colab) with the given hyperparameters in the `model_training.py` file. The saved weights were around 590 MB per epoch. You can expect it to take much longer on a less powerful GPU. Please plan your resources accordingly. Note, the default GPU on Colab is Tesla T4.\n",
    "\n",
    "Feel free to stop the training anytime you think your model is trained enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2035e08f3b2a9ea4",
   "metadata": {
    "id": "2035e08f3b2a9ea4"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "from model_training import load_trained_model, ModelHyperParams, TrainingHyperParams\n",
    "training_hp = TrainingHyperParams()\n",
    "model_hp = ModelHyperParams()\n",
    "print('Training Hyperparameters:', training_hp)\n",
    "print('---------------------------------------------------------------------------')\n",
    "print('Model Hyperparameters:', model_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b49dec",
   "metadata": {
    "id": "44b49dec"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# TODO: train the model\n",
    "if device == 'cuda':\n",
    "  torch.cuda.empty_cache()\n",
    "training_hp = TrainingHyperParams()\n",
    "model_hp = ModelHyperParams()\n",
    "model = load_trained_model(training_hp, model_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2cbde000b87518",
   "metadata": {
    "collapsed": false,
    "id": "4e2cbde000b87518"
   },
   "source": [
    "## 4. Decoding/Sampling [27 points + 15 bonus points - Programming]\n",
    "\n",
    "In the previous section, we implemented and trained the Transformer model. Now, we will implement the decoding/sampling mechanism to generate the output sequence given the input sequence using the trained model.\n",
    "\n",
    "The sampling process involves generating the output sequence token by token. At each step, the model predicts the next token in the sequence based on the input sequence and the tokens generated so far. The predicted token is then appended to the output sequence, and the process is repeated until an end-of-sequence token is generated or the maximum sequence length is reached. This technique is transferable across various sequence-to-sequence tasks, including machine translation, text summarization, open-ended generation, question-answering, etc. Different strategies navigate this balance in unique ways, impacting the model's output in terms of variability, unpredictability, and fidelity to human-like language patterns.\n",
    "\n",
    "Here is a great reference blog on Sampling Techniques - https://huggingface.co/blog/how-to-generate\n",
    "\n",
    "In this assignment, we will implement 5 decoding strategies, the implementation of which will be done in `sequence_generator.py` file. They are as follows -\n",
    "- Greedy Sampling\n",
    "- Random Sampling\n",
    "- Top-k Sampling\n",
    "- Top-p Sampling\n",
    "- Beam Search\n",
    "\n",
    "First we will implement the logic of each of the strategy given a probability distribution, and then move to utilizing those functions to decode the output sequence using the trained Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3998a5c723b9956",
   "metadata": {
    "collapsed": false,
    "id": "a3998a5c723b9956"
   },
   "source": [
    "### 4.1. Greedy Sampling [4 points - Programming]\n",
    "\n",
    "Greedy sampling is a simple decoding strategy where the token with the highest probability is selected at each step. This strategy is computationally efficient but may lead to suboptimal results as it does not consider the uncertainty in the model's predictions.\n",
    "\n",
    "Implement the `sample_greedy` function in `sequence_generator.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a547b3f3aa8c32e",
   "metadata": {
    "id": "2a547b3f3aa8c32e"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from sequence_generator import SequenceGenerator\n",
    "\n",
    "probs = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.33, 0.32, 0.2, 0.15]], dtype=torch.float32) # sum of each row is 1\n",
    "token_ids, log_probs = SequenceGenerator.sample_greedy(probs)\n",
    "expected_token_ids = torch.tensor([3, 0], dtype=torch.long)\n",
    "expected_log_probs = torch.tensor([-0.9163, -1.1087], dtype=torch.float32)\n",
    "assert torch.allclose(token_ids, expected_token_ids), \"Test Failed: Incorrect token_ids.\"\n",
    "assert torch.allclose(log_probs, expected_log_probs, atol=1e-4), \"Test Failed: Incorrect log_probs.\"\n",
    "print(\"Greedy Sampling - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f706a754731e176",
   "metadata": {
    "collapsed": false,
    "id": "9f706a754731e176"
   },
   "source": [
    "### 4.2. Random Sampling [6 points - Programming]\n",
    "\n",
    "Random sampling, selects the next word purely based on its predicted probability, allowing for a high degree of diversity but at the risk of producing less coherent sentences. It involves selecting the next word in a sequence based on a probability distribution over the vocabulary. This distribution is derived from the model's predictions, indicating how likely each word is to follow the given context. Here's a mathematical breakdown of how random sampling works:\n",
    "\n",
    "Given a sequence of words $S = \\{s_1, s_2, \\ldots, s_{n-1}\\}$, an NLP model predicts the next word $s_n$ by estimating the probability distribution $P(V|S)$ over the vocabulary $V = \\{v_1, v_2, \\ldots, v_M\\}$, where $M$ is the size of the vocabulary. The model outputs a probability $P(v_i|S)$ for each word $v_i$ in the vocabulary, indicating the likelihood of $v_i$ being the next word. These probabilities are normalized so that they sum up to 1:\n",
    "\n",
    "$$\\sum_{i=1}^{M} P(v_i|S) = 1$$\n",
    "\n",
    "Random Sampling involves selecting the next word $s_n$ from the vocabulary $V$ based on the predicted probabilities. Specifically, each word $v_i$ is chosen with a probability $P(v_i|S)$. This selection process can be conceptualized as a random variable $X$ with a multinomial distribution, where the probability mass function (PMF) for $X = v_i$ (the event of choosing word $v_i$ as the next word) is given by:\n",
    "\n",
    "$$P(X = v_i) = P(v_i|S)$$\n",
    "\n",
    "This process is repeated for each new word in the sequence, with the context $S$ being updated to include the newly chosen words, until a termination condition is met (e.g., a maximum sequence length or the selection of a special end-of-sequence token).\n",
    "\n",
    "Implement the `sample_random` function in `sequence_generator.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267dd8fc1e50dc14",
   "metadata": {
    "id": "267dd8fc1e50dc14"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from sequence_generator import SequenceGenerator\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "logit = torch.tensor([[10, -10, -10, -5], [10, 50, 11, 5]], dtype=torch.float32)\n",
    "even_logit = torch.tensor([[10, 10, 10, 10], [100, 100, 10, 10]], dtype=torch.float32)\n",
    "probs = softmax(logit, dim=-1)\n",
    "even_probs = softmax(even_logit, dim=-1)\n",
    "token_ids, log_probs = SequenceGenerator.sample_random(probs)\n",
    "expected_token_ids = torch.tensor([0, 1], dtype=torch.long)\n",
    "expected_log_probs = torch.tensor([-3.5763e-07,  0.0000e+00], dtype=torch.float32)\n",
    "assert torch.allclose(token_ids, expected_token_ids), \"Test Failed: Incorrect token_ids for Test 1.\"\n",
    "assert torch.allclose(log_probs, expected_log_probs, atol=1e-9), \"Test Failed: Incorrect log_probs for Test 1.\"\n",
    "\n",
    "token_ids, log_probs = SequenceGenerator.sample_random(even_probs)\n",
    "expected_log_probs = torch.tensor([-1.3863, -0.6931], dtype=torch.float32)\n",
    "assert torch.allclose(log_probs, expected_log_probs, atol=1e-4), \"Test Failed: Incorrect log_probs for Test 2.\"\n",
    "print(\"Random Sampling - All tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60302d5dfbbbab97",
   "metadata": {
    "collapsed": false,
    "id": "60302d5dfbbbab97"
   },
   "source": [
    "### 4.3. Top-k Sampling [6 points - Programming]\n",
    "\n",
    "Top-k sampling is a decoding strategy that involves selecting the next word from the top k most likely words in the probability distribution. This technique balances the diversity of random sampling with the stability of greedy sampling by restricting the selection to a subset of the vocabulary. The top-k sampling strategy is particularly useful in scenarios where the model's predictions are uncertain, as it allows for a controlled exploration of the vocabulary space.\n",
    "\n",
    "Given a context sequence $S = \\{s_1, s_2, \\ldots, s_{n-1}\\}$, and a vocabulary $V = \\{v_1, v_2, \\ldots, v_M\\}$, the model computes a probability distribution $P(V|S)$, predicting the likelihood of each word in the vocabulary being the next appropriate word.\n",
    "\n",
    "In top-k sampling, the model first identifies the subset $V_k \\subseteq V$ consisting of the top $k$ words with the highest probabilities. Mathematically, this subset is defined as:\n",
    "\n",
    "$$\n",
    "V_k = \\{v | P(v|S) \\text{ is among the top } k \\text{ of all } P(v_i|S), \\forall v_i \\in V\\}\n",
    "$$\n",
    "\n",
    "The probability distribution is then modified to focus solely on these $k$ words, with probabilities outside this set being set to zero. The modified distribution $P'(V|S)$ is defined as:\n",
    "\n",
    "$$\n",
    "P'(v_i|S) =\n",
    "\\begin{cases}\n",
    "P(v_i|S) & \\text{if } v_i \\in V_k \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The probabilities in $P'(V|S)$ are then renormalized so that they sum to 1, ensuring a valid probability distribution. The next word is sampled from this adjusted distribution, effectively narrowing the selection to the most probable candidates and discarding less likely, potentially noisy options.\n",
    "\n",
    "The selection of the next word $s_n$ is, therefore, a random variable $X$ with a distribution reflecting the adjusted probabilities:\n",
    "\n",
    "$$\n",
    "P(X = v_i) = \\frac{P'(v_i|S)}{\\sum_{v_j \\in V_k} P'(v_j|S)}\n",
    "$$\n",
    "\n",
    "Top-k sampling balances the diversity of text generation with the coherence of the generated sequences, making it a preferred choice in scenarios where quality and readability of the generated text are crucial.\n",
    "\n",
    "You can refer [this paper](https://arxiv.org/pdf/1805.04833.pdf) for more on Top-k Sampling.\n",
    "\n",
    "\n",
    "Implement the `sample_top_k` function in `sequence_generator.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee8474dffb9946",
   "metadata": {
    "id": "62ee8474dffb9946"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from sequence_generator import SequenceGenerator\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "logit = torch.tensor([[10, -10, -10, -5], [10, 50, 11, 5]], dtype=torch.float32)\n",
    "even_logit = torch.tensor([[10, 10, 10, 10], [100, 100, 10, 10]], dtype=torch.float32)\n",
    "probs = softmax(logit, dim=-1)\n",
    "even_probs = softmax(even_logit, dim=-1)\n",
    "token_ids, log_probs = SequenceGenerator.sample_top_k(probs, k=3)\n",
    "expected_token_ids = torch.tensor([0, 1], dtype=torch.long)\n",
    "expected_log_probs = torch.tensor([-2.9802e-07,  0.0000e+00], dtype=torch.float32)\n",
    "assert torch.allclose(token_ids, expected_token_ids), \"Test Failed: Incorrect token_ids for Test 1.\"\n",
    "assert torch.allclose(log_probs, expected_log_probs, atol=1e-9), \"Test Failed: Incorrect log_probs for Test 1.\"\n",
    "\n",
    "token_ids, log_probs = SequenceGenerator.sample_top_k(even_probs, k=3)\n",
    "expected_log_probs = torch.tensor([[-1.0986, -0.6931]], dtype=torch.float32)\n",
    "assert torch.allclose(log_probs, expected_log_probs, atol=1e-4), \"Test Failed: Incorrect log_probs for Test 2.\"\n",
    "\n",
    "print(\"Top-k Sampling - All tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15baec07da9934f9",
   "metadata": {
    "collapsed": false,
    "id": "15baec07da9934f9"
   },
   "source": [
    "### 4.4. Top-p Sampling [6 points - Programming]\n",
    "\n",
    "Top-p sampling, also known as nucleus sampling, is a decoding strategy that involves selecting the next word from the smallest set of words whose cumulative probability mass exceeds a predefined threshold p.\n",
    "\n",
    "It is an adaptive technique that balances the trade-off between randomness and determinism. Unlike top-k sampling that selects the top $k$ most probable words, top-p sampling dynamically selects a subset of the vocabulary by choosing the smallest set of words whose cumulative probability exceeds a threshold $p$. This method focuses on the \"nucleus\" of probable words, discarding the tail of less likely words to ensure both diversity and coherence in the generated text.\n",
    "\n",
    "Given a sequence $S = \\{s_1, s_2, \\ldots, s_{n-1}\\}$ and a vocabulary $V = \\{v_1, v_2, \\ldots, v_M\\}$, the model first computes the probability distribution $P(V|S)$ for the next word. The words in $V$ are then sorted by their probability in descending order, resulting in a sorted list $V_{sorted} = \\{v_{(1)}, v_{(2)}, \\ldots, v_{(M)}\\}$ where $P(v_{(1)}|S) \\ge P(v_{(2)}|S) \\ge \\ldots \\ge P(v_{(M)}|S)$.\n",
    "\n",
    "To determine the subset $V_p \\subseteq V$ for sampling, we find the smallest $k$ such that:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k} P(v_{(i)}|S) > p\n",
    "$$\n",
    "\n",
    "The cumulative probability distribution is then adjusted to zero out the probabilities of words not included in $V_p$, and the remaining probabilities are renormalized to sum to 1. The next word $s_n$ is sampled from this adjusted distribution.\n",
    "\n",
    "The selection of $s_n$ can be represented as sampling from a random variable $X$ with the modified distribution:\n",
    "\n",
    "$$\n",
    "P'(X = v_{(i)}) = \\frac{P(v_{(i)}|S)}{\\sum_{j=1}^{k} P(v_{(j)}|S)}, \\text{ for } i \\le k\n",
    "$$\n",
    "\n",
    "Nucleus sampling's dynamic nature allows it to automatically adjust the breadth of the sampling space based on the context, making it particularly effective for generating high-quality and varied text outputs. This method has become a popular choice in state-of-the-art NLP models for tasks such as text completion, storytelling, and dialogue generation.\n",
    "\n",
    "You can find more on Top-p Sampling in [this paper](https://arxiv.org/abs/1904.09751).\n",
    "\n",
    "Implement the `sample_top_p` function in `sequence_generator.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279658a29bdbd9d",
   "metadata": {
    "id": "8279658a29bdbd9d"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from sequence_generator import SequenceGenerator\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "logit = torch.tensor([[10, -10, -10, -5], [10, 50, 11, 5]], dtype=torch.float32)\n",
    "even_logit = torch.tensor([[10, 10, 10, 10], [100, 100, 10, 10]], dtype=torch.float32)\n",
    "\n",
    "probs = softmax(logit, dim=-1)\n",
    "even_probs = softmax(even_logit, dim=-1)\n",
    "token_ids, log_probs = SequenceGenerator.sample_top_p(probs, p=0.5)\n",
    "expected_token_ids = torch.tensor([0, 1], dtype=torch.long)\n",
    "expected_log_probs = torch.tensor([0.0000e+00,  0.0000e+00], dtype=torch.float32)\n",
    "assert torch.allclose(token_ids, expected_token_ids), \"Test Failed: Incorrect token_ids for Test 1.\"\n",
    "assert torch.allclose(log_probs, expected_log_probs, atol=1e-9), \"Test Failed: Incorrect log_probs for Test 1.\"\n",
    "\n",
    "token_ids, log_probs = SequenceGenerator.sample_top_p(even_probs, p=0.5)\n",
    "expected_log_probs = torch.tensor([-1.0986, -0.6931], dtype=torch.float32)\n",
    "assert torch.allclose(log_probs, expected_log_probs, atol=1e-4), \"Test Failed: Incorrect log_probs for Test 2.\"\n",
    "\n",
    "print(\"Top-p Sampling - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a64bd306aaf3d",
   "metadata": {
    "collapsed": false,
    "id": "151a64bd306aaf3d"
   },
   "source": [
    "### 4.5. Generate Function [5 points - Programming]\n",
    "\n",
    "Now that we have implemented the decoding strategies, we can use them to generate the output sequence given the input sequence using the trained Transformer model. The `generate` function in `sequence_generator.py` will take the input sequence, the trained model, and the decoding strategy as input and return the generated output sequence. It will use the functions you implemented earlier to sample the next token at each step based on the model's predictions.\n",
    "\n",
    "Steps to implement the `generate` function:\n",
    "1. Pass the input sequence through the encoder to get the memory.\n",
    "2. Initialize the output sequence with the start token.\n",
    "3. Loop until the end token is generated or the maximum sequence length is reached:\n",
    "    - Pass the output sequence and memory through the decoder to get the next token probabilities.\n",
    "    - Use the specified decoding strategy to sample the next token.\n",
    "    - Append the sampled token to the output sequence.\n",
    "4. Return the generated output sequence.\n",
    "5. Make sure to handle the padding tokens in the output sequence.\n",
    "\n",
    "Implement the `generate` function in `sequence_generator.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff1d0446efac88",
   "metadata": {
    "id": "2eff1d0446efac88"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "from sequence_generator import SequenceGenerator, DecodingStrategy\n",
    "from dummy_transformer import DummyTransformer, DummyGenerator\n",
    "generator = DummyGenerator()\n",
    "transformer = DummyTransformer(generator)\n",
    "inp = torch.tensor([[1, 2, 3, 4], [1, 2, 4, 2], [2, 4, 3, 1]], dtype=torch.float32)\n",
    "sg = SequenceGenerator(model=transformer, sos_token=1, eos_token=2, pad_token=0, max_len=5)\n",
    "src_mask = (inp!=0)\n",
    "res = sg.generate(inp, src_mask, strategy=DecodingStrategy.GREEDY)\n",
    "desired_result = [[1, 3, 3, 3, 3], [1, 2], [1, 1, 1, 1, 1]]\n",
    "assert res == desired_result, \"Test Failed: Incorrect output sequence for Generate Function.\"\n",
    "print(\"Generate Function - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc7c1256463930",
   "metadata": {
    "collapsed": false,
    "id": "d9bc7c1256463930"
   },
   "source": [
    "### 4.6. Beam Search [4 points - Programming] (Bonus)\n",
    "\n",
    "Beam search is an advanced strategy for generating text sequences, striking a balance between the brute-force exploration of all possible sequences and the narrow focus of greedy sampling. At each step in generating a sequence, given a context $S = \\{s_1, s_2, \\ldots, s_{n-1}\\}$, the algorithm maintains a set of $b$ candidate sequences, called \"beams\", each with its own cumulative probability. The beam width $b$ controls the number of sequences explored in parallel, with each step extending each candidate sequence by one word and keeping only the top $b$ sequences according to their cumulative probabilities:\n",
    "\n",
    "$$\n",
    "B_n = \\text{Top}_b \\left\\{ B_{n-1} \\oplus v_i : P(S \\oplus v_i|S) \\right\\}\n",
    "$$\n",
    "\n",
    "where $B_n$ represents the set of beams at step $n$, $\\oplus$ denotes sequence concatenation, and $\\text{Top}_b$ selects the $b$ sequences with the highest cumulative probability. The probability of a sequence is typically the product of the probabilities of its constituent words, adjusted for sequence length to prevent bias toward shorter sequences. Beam search continues this process until a stopping condition is reached, such as all beams ending with an end-of-sequence token or reaching a predetermined sequence length.\n",
    "\n",
    "While beam search significantly improves the likelihood of finding high-quality sequences by considering multiple hypotheses, its computational cost increases with the beam width. Moreover, despite exploring more options than greedy sampling, beam search can still favor shorter sequences due to its cumulative probability calculation and may converge to similar sequences within the beams, reducing diversity. Nevertheless, it remains a popular choice in NLP tasks where the balance between output quality and computational efficiency is crucial, offering a pragmatic compromise between exhaustive search and deterministic selection.\n",
    "\n",
    "Beam search is used in sampling too. You can refer to the article in the Sampling section of this assignment for more details on beam search.\n",
    "\n",
    "You can also refer this video - https://www.youtube.com/watch?v=RLWuzLLSIgw\n",
    "\n",
    "Implement the `beam_search` function in `sequence_generator.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338e57de91edd7f",
   "metadata": {
    "id": "e338e57de91edd7f"
   },
   "outputs": [],
   "source": [
    "# LOCAL TESTS - DO NOT CHANGE THIS CELL\n",
    "# Please note that this is a heurestic test by skewing the probabilities to get definitive results.\n",
    "# Simulating a real unit test is not possible due to randomization elements and training dependencies.\n",
    "# Also it can't validate if you are implementing the beam width logic correctly.\n",
    "# We may evaluate your code manually later for full credits and change scores if necessary\n",
    "from sequence_generator import SequenceGenerator\n",
    "from dummy_transformer import DummyTransformer, DummyGenerator\n",
    "generator = DummyGenerator()\n",
    "transformer = DummyTransformer(generator)\n",
    "src = torch.tensor([[7, 7, 3, 4, 5, 6, 7, 8, 9, 1000]], dtype=torch.long)\n",
    "sg = SequenceGenerator(model=transformer, sos_token=1, eos_token=2, pad_token=0, max_len=5)\n",
    "src_mask = (src!=0)\n",
    "res = sg.beam_search(src, src_mask, beam_width=1)\n",
    "expected_output = torch.tensor([1, 9, 9, 9, 9])\n",
    "assert torch.allclose(torch.tensor(res), expected_output), \"Test Failed: Incorrect beam search output.\"\n",
    "\n",
    "sg = SequenceGenerator(model=transformer, sos_token=3, eos_token=2, pad_token=0, max_len=7)\n",
    "res = sg.beam_search(src, src_mask, beam_width=2)\n",
    "expected_output = torch.tensor([3, 9, 9, 9, 9, 9, 9])\n",
    "assert torch.allclose(torch.tensor(res), expected_output), \"Test Failed: Incorrect beam search output.\"\n",
    "print(\"Beam Search - All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89498ad2e2707d8c",
   "metadata": {
    "collapsed": false,
    "id": "89498ad2e2707d8c"
   },
   "source": [
    "### 4.7. BLEU Score Evaluation using Sampling Techniques [11 points - Programming] (Bonus)\n",
    "\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate the quality of machine-translated text. It compares the machine-generated text to one or more reference translations and assigns a score based on the n-gram overlap between the generated text and the references. BLEU scores range from 0 to 1, with higher scores indicating better translation quality. We will scale BLEU score by 100 to get the percentage.\n",
    "\n",
    "We already provide the implementation of BLEU score. If you don't get desired results in the following cell, you can refer to the implementation in `utils.py`. Check if your model, decoding techniques are correctly implemented. Due to the stochastic nature of sampling techniques, the forward pass tests may not always be correct. Further, due to the randomness in sampling, the BLEU score may vary slightly each time you run the cell.\n",
    "\n",
    "Read more about BLEU Score in [this paper](https://aclanthology.org/P02-1040/).\n",
    "\n",
    "Sampling with different techniques require different hyperparameters. You can adjust the hyperparameters in the following cells to get the best BLEU score. Since these hyperparameters are new, we have left them for you to fill. But, we have provided some recommendations so that you're not lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f33eb37878083",
   "metadata": {
    "collapsed": false,
    "id": "5a5f33eb37878083"
   },
   "source": [
    "#### 4.7.1. BLEU Score Evaluation using Greedy Sampling [2 points - Programming] (Bonus)\n",
    "\n",
    "\n",
    "You will get 0-2 points based on the BLEU score obtained using Greedy Sampling.\n",
    "Following are the score ranges -\n",
    "- 0-10: 0 points\n",
    "- 10-20: 1 point\n",
    "- 20+: 2 points\n",
    "\n",
    "Recommended value of `max_length` is 40-100 and `batch_size_for_decoding` is 64. You can adjust these values to get the best BLEU score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8144139b5bc2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:16:55.290094Z",
     "start_time": "2024-10-02T23:16:55.146371Z"
    },
    "id": "27c8144139b5bc2d"
   },
   "outputs": [],
   "source": [
    "# Specify decoding hyperparameters here\n",
    "# TODO: Select hyperparameters for greedy sampling\n",
    "max_length = None\n",
    "model_weights_path = None # Change this to the path of the trained model weights you want to use (eg. model_epoch_10.pt)\n",
    "folder_path = os.path.join(os.getcwd(), 'model_weights')\n",
    "full_weights_path = os.path.join(folder_path, model_weights_path)\n",
    "batch_size_for_decoding = 64 # adjust if facing memory issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba91eab062b8dc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:18:15.885526Z",
     "start_time": "2024-10-02T23:16:55.437932Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cba91eab062b8dc5",
    "outputId": "657a000b-8ad3-43e1-acfe-f70f210b0826"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "from bleu_score import get_bleu_score\n",
    "from sequence_generator import DecodingStrategy\n",
    "preds, refs, bleu_score = get_bleu_score(full_weights_path, device, max_len=max_length, decoding_strategy=DecodingStrategy.GREEDY, batch_size=batch_size_for_decoding)\n",
    "print('BLEU Score using Greedy Decoding:', bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a00ebac8ef1a4",
   "metadata": {
    "id": "1d0a00ebac8ef1a4"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Generate submission files for the test set (don't judge anything from BLEU score - we are not releasing references, they will be evaluated on Gradescope)\n",
    "from bleu_score import generate_test_set_predictions\n",
    "from sequence_generator import DecodingStrategy\n",
    "generate_test_set_predictions(full_weights_path, device, 'output_greedy', max_len=max_length, decoding_strategy=DecodingStrategy.GREEDY, batch_size=batch_size_for_decoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f97d1bd16bd7ce1",
   "metadata": {
    "collapsed": false,
    "id": "5f97d1bd16bd7ce1"
   },
   "source": [
    "#### 4.7.2. BLEU Score Evaluation using Random Sampling [2 points - Programming] (Bonus)\n",
    "\n",
    "You will get 0-2 points based on the BLEU score obtained using Random Sampling.\n",
    "Following are the score ranges -\n",
    "- 0-5: 0 points\n",
    "- 5-10: 1 point\n",
    "- 10+: 2 points\n",
    "\n",
    "Recommended value of `max_length` is 40-100 and `batch_size_for_decoding` is 64. You can adjust these values to get the best BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ce636107dd23",
   "metadata": {
    "id": "324ce636107dd23"
   },
   "outputs": [],
   "source": [
    "# Specify decoding hyperparameters here\n",
    "# TODO: Select hyperparameters for random sampling\n",
    "max_length = None\n",
    "model_weights_path = None # Change this to the path of the trained model weights you want to use (eg. model_epoch_10.pt)\n",
    "folder_path = os.path.join(os.getcwd(), 'model_weights')\n",
    "full_weights_path = os.path.join(folder_path, model_weights_path)\n",
    "batch_size_for_decoding = 64 # adjust if facing memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25deefca9923408",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25deefca9923408",
    "outputId": "2353ecd3-b392-4314-85ef-30e8bca7c5a0"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "from bleu_score import get_bleu_score\n",
    "from sequence_generator import DecodingStrategy\n",
    "preds, refs, bleu_score = get_bleu_score(full_weights_path, device, max_len=max_length, decoding_strategy=DecodingStrategy.RANDOM, batch_size=batch_size_for_decoding)\n",
    "print('BLEU Score using Random Sampling:', bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fabdc036ed545b",
   "metadata": {
    "id": "64fabdc036ed545b"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Generate submission files for the test set (don't judge anything from BLEU score - we are not releasing references, they will be evaluated on Gradescope)\n",
    "from bleu_score import generate_test_set_predictions\n",
    "from sequence_generator import DecodingStrategy\n",
    "generate_test_set_predictions(full_weights_path, device, 'output_random', max_len=max_length, decoding_strategy=DecodingStrategy.RANDOM, batch_size=batch_size_for_decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a528dd64dec9a9",
   "metadata": {
    "collapsed": false,
    "id": "c3a528dd64dec9a9"
   },
   "source": [
    "#### 4.7.3. BLEU Score Evaluation using Top-k Sampling [2 points - Programming] (Bonus)\n",
    "\n",
    "You will get 0-2 points based on the BLEU score obtained using Top-k Sampling.\n",
    "Following are the score ranges -\n",
    "- 0-10: 0 points\n",
    "- 10-16: 1 point\n",
    "- 16+: 2 points\n",
    "\n",
    "Recommended value of `max_length` is 40-100, `batch_size_for_decoding` is 50 and `k` is 3-6. You can adjust these values to get the best BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477aad03a48edbec",
   "metadata": {
    "id": "477aad03a48edbec"
   },
   "outputs": [],
   "source": [
    "# Specify decoding hyperparameters here\n",
    "# TODO: Select hyperparameters for top-k sampling\n",
    "max_length = None\n",
    "k = None\n",
    "model_weights_path = None # Change this to the path of the trained model weights you want to use (eg. model_epoch_10.pt)\n",
    "folder_path = os.path.join(os.getcwd(), 'model_weights')\n",
    "full_weights_path = os.path.join(folder_path, model_weights_path)\n",
    "batch_size_for_decoding = 50 # adjust if facing memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb711af28a6a062",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bb711af28a6a062",
    "outputId": "539afd3f-d103-4139-c222-f1f116f119d1"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "from bleu_score import get_bleu_score\n",
    "from sequence_generator import DecodingStrategy\n",
    "preds, refs, bleu_score = get_bleu_score(full_weights_path, device, max_len=max_length, decoding_strategy=DecodingStrategy.TOP_K, batch_size=batch_size_for_decoding, k=k)\n",
    "print('BLEU Score using Top-k Sampling:', bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c52e094c277ae6",
   "metadata": {
    "id": "83c52e094c277ae6"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Generate submission files for the test set (don't judge anything from BLEU score - we are not releasing references, they will be evaluated on Gradescope)\n",
    "from bleu_score import generate_test_set_predictions\n",
    "from sequence_generator import DecodingStrategy\n",
    "generate_test_set_predictions(full_weights_path, device, 'output_top_k', max_len=max_length, decoding_strategy=DecodingStrategy.TOP_K, batch_size=batch_size_for_decoding, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a2b8239bbee79",
   "metadata": {
    "collapsed": false,
    "id": "e95a2b8239bbee79"
   },
   "source": [
    "#### 4.7.4. BLEU Score Evaluation using Top-p Sampling [2 points - Programming] (Bonus)\n",
    "\n",
    "You will get 0-2 points based on the BLEU score obtained using Top-p Sampling.\n",
    "Following are the score ranges -\n",
    "- 0-10: 0 points\n",
    "- 10-16: 1 point\n",
    "- 16+: 2 points\n",
    "\n",
    "Recommended value of `max_length` is 40-100, `batch_size_for_decoding` is 50 and `p` is 0.5-0.85. You can adjust these values to get the best BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67d790c4dfc5d8",
   "metadata": {
    "id": "9a67d790c4dfc5d8"
   },
   "outputs": [],
   "source": [
    "# Specify decoding hyperparameters here\n",
    "# TODO: Select hyperparameters for top-p sampling\n",
    "max_length = None\n",
    "p = None\n",
    "model_weights_path = None # Change this to the path of the trained model weights you want to use (eg. model_epoch_10.pt)\n",
    "folder_path = os.path.join(os.getcwd(), 'model_weights')\n",
    "full_weights_path = os.path.join(folder_path, model_weights_path)\n",
    "batch_size_for_decoding = 50 # adjust if facing memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32018cee0fca58b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d32018cee0fca58b",
    "outputId": "36e72990-ca80-4c95-f818-a27ef8f0b825"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "from bleu_score import get_bleu_score\n",
    "from sequence_generator import DecodingStrategy\n",
    "preds, refs, bleu_score = get_bleu_score(full_weights_path, device, max_len=max_length, decoding_strategy=DecodingStrategy.TOP_P, batch_size=batch_size_for_decoding, p=p)\n",
    "print('BLEU Score using Top-p Sampling:', bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9169db0b286daf8",
   "metadata": {
    "id": "e9169db0b286daf8"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Generate submission files for the test set (don't judge anything from BLEU score - we are not releasing references, they will be evaluated on Gradescope)\n",
    "from bleu_score import generate_test_set_predictions\n",
    "from sequence_generator import DecodingStrategy\n",
    "generate_test_set_predictions(full_weights_path, device, 'output_top_p', max_len=max_length, decoding_strategy=DecodingStrategy.TOP_P, batch_size=batch_size_for_decoding, p=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0e40e8ae810e9",
   "metadata": {
    "collapsed": false,
    "id": "24d0e40e8ae810e9"
   },
   "source": [
    "#### 4.7.5. BLEU Score Evaluation using Beam Search [3 points - Programming] (Bonus)\n",
    "\n",
    "You will get 0-2 points based on the BLEU score obtained using Beam Search.\n",
    "Following are the score ranges -\n",
    "- 0-10: 0 points\n",
    "- 10-15: 1 point\n",
    "- 15-19: 2 points\n",
    "- 19+: 3 points\n",
    "\n",
    "Recommended value of `max_length` is 40-100, and `beam_width` is 2-6. You can adjust these values to get the best BLEU score.\n",
    "\n",
    "Since beam search is not batched, we will evaluate it only with first 1000 data samples. So, don't compare it with other metrics directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pwqN_4JetfVw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T01:52:52.500102Z",
     "start_time": "2024-10-03T01:52:52.461953Z"
    },
    "id": "pwqN_4JetfVw"
   },
   "outputs": [],
   "source": [
    "# Specify decoding hyperparameters here\n",
    "# TODO: Select hyperparameters for beam search\n",
    "beam_width = None\n",
    "max_length = None\n",
    "model_weights_path = None # Change this to the path of the trained model weights you want to use (eg. model_epoch_10.pt)\n",
    "folder_path = os.path.join(os.getcwd(), 'model_weights')\n",
    "full_weights_path = os.path.join(folder_path, model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1ccbb51712941",
   "metadata": {
    "id": "91f1ccbb51712941"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "from bleu_score import get_bleu_score\n",
    "from sequence_generator import DecodingStrategy\n",
    "preds, refs, bleu_score = get_bleu_score(full_weights_path, device, max_len=max_length, decoding_strategy=DecodingStrategy.BEAM_SEARCH, beam_width=beam_width)\n",
    "print('BLEU Score using Beam Search:', bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea68b317f0ba78",
   "metadata": {
    "id": "8aea68b317f0ba78"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Generate submission files for the test set (don't judge anything from BLEU score - we are not releasing references, they will be evaluated on Gradescope)\n",
    "from bleu_score import generate_test_set_predictions\n",
    "from sequence_generator import DecodingStrategy\n",
    "generate_test_set_predictions(full_weights_path, device, 'output_beam_search', max_len=max_length, decoding_strategy=DecodingStrategy.BEAM_SEARCH, beam_width=beam_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8a281b021451c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Greedy decoding** tends to maximize the immediate likelihood at each step, which might explain why it gives the best BLEU score in this case. BLEU evaluates how closely the generated sequences match the reference ones, rewarding exact matches and overlaps in n-grams. Since greedy decoding produces deterministic outputs by selecting the most probable token at each step, it is likely to generate more predictable, frequent phrases that align well with reference sequences, which BLEU heavily rewards. Since the model is trained on similar distribution, this becomes all the more expected.\n",
    "\n",
    "**Top-k** and **top-p sampling** introduce more diversity into the sequence generation by sampling tokens based on probability distributions, often leading to more creative or diverse outputs. However, this diversity can hurt BLEU scores as the generated text becomes less predictable, thus deviating from the reference. **Random sampling**, as expected, introduces the most variability and often generates outputs that are highly divergent from the reference, lowering BLEU scores.\n",
    "\n",
    "Despite performing well on BLEU, greedy decoding has limitations:\n",
    "\n",
    "1. **Suboptimal Output**: Greedy decoding only considers local maxima, which can lead to suboptimal outputs globally. It might miss better sequences that can only be identified by exploring alternative paths.\n",
    "   \n",
    "2. **Lack of Diversity**: Greedy decoding lacks variability, which can limit its usefulness in tasks where diverse or creative outputs are important (e.g., dialogue generation or story generation).\n",
    "\n",
    "3. **Misleading BLEU Scores**: BLEU score, while useful, is not the perfect measure of generation quality. It may not fully capture semantic fidelity, fluency, or overall coherence, which might be better optimized by beam search or sampling techniques.\n",
    "\n",
    "While greedy decoding can score well on BLEU, it may not be the best choice for tasks where creativity, diversity, or exploring multiple generation paths is crucial. There are some metrics which evaluate the text for similarity in embedding space, fluency, coherance, etc. In these cases, other decoding strategies like beam search or top-k/top-p will likely perform closer to, or better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddccf0766644cb6",
   "metadata": {
    "collapsed": false,
    "id": "fddccf0766644cb6"
   },
   "source": [
    "## 5. Analysis [20 points - Non-Programming]\n",
    "\n",
    "Analysis questions will be graded manually. Please answer them with the best of your knowledge. There is no right or wrong answer, but you will be graded based on the depth of your answer. You can refer to the results obtained in the previous sections to support your answers. Also, make sure your code cells are clear and complete. They should not get cropped when converted to PDF. To do this, don't write too long lines, and split them if necessary. You can add more code or markdown cells if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b86ff73c6f8fd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.1. Visualizing Encoder Self-Attention Map [5 points - Non-Programming]    \n",
    "\n",
    "Consider the following de-en translation -\n",
    "German: \"Der Wissenschaftler, der das neue Verfahren entwickelt hat, wird morgen auf der Konferenz sprechen.\"\n",
    "English: \"The scientist who developed the new method will speak at the conference tomorrow.\"\n",
    "\n",
    "Using your trained transformer, visualize the attention weights for the first layer and the last layer of the encoder. You will have multiple attention heads, display all of them as heatmap, for both the layers. Clearly label the heatmap. You should have `2*num_heads` subplots, where the two rows represent two layers (add the index of the layer), and each column represent an attention head. In each subplot, have a heatmap denoting the attention matrix. \n",
    "\n",
    "Explain the patterns you observe in the attention weights. What can you infer about the input and model's learning process from these attention weights?\n",
    "\n",
    "**How to do this?** \n",
    "- Each encoder will have encoder blocks. Take the first and the last encoder block. \n",
    "- In each encoder block, you should have an attention layer, and in each attention layer, you should have an attention map for each head, which you would have initialized in `self.attn` class-level variable using the second return value of `attention` function. \n",
    "- You essentially need to do a forward pass using the german sentence (after tokenizing), and take this attention map for the above two layers. The output should be heatmap plot of `german_sequence_length x german_sequence_length` for each head in each layer. You can use `seaborn` or `matplotlib` for this visualization.\n",
    "\n",
    "You can add as many code or markdown cells as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a1090cd83398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T03:10:08.893345Z",
     "start_time": "2024-10-03T03:10:08.824907Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the visualization of attention weights for the first and last layer of the encoder\n",
    "# YOUR CODE HERE\n",
    "# Example Heatmap Generation Code\n",
    "# matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# # Display heatmap using Matplotlib\n",
    "# plt.imshow(matrix, cmap='hot', interpolation='nearest')\n",
    "# plt.colorbar()  # Add a color bar to show the scale\n",
    "# plt.title('Heatmap using Matplotlib')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb740eb69e6c598",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff97f48ca5cc51",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.2. Visualizing Encoder-Decoder Cross-Attention Map [5 points - Non-Programming]\n",
    "\n",
    "Use the same translation as in the previous question.\n",
    "German: \"Der Wissenschaftler, der das neue Verfahren entwickelt hat, wird morgen auf der Konferenz sprechen.\"\n",
    "English: \"The scientist who developed the new method will speak at the conference tomorrow.\"\n",
    "\n",
    "Using your trained transformer, visualize the attention weights for the cross-attention layer for the first and last layer of the decoder. You will have multiple attention heads, display all of them as heatmap, for both the layers. Clearly label the heatmap. You should have `2*num_heads` subplots, where the two rows represent two layers (add the index of the layer), and each column represent an attention head. In each subplot, have a heatmap denoting the attention matrix.\n",
    "\n",
    "Explain the patterns you observe in the attention weights. What can you infer about the input and model's learning process from these attention weights?\n",
    "\n",
    "**How to do this?** \n",
    "- Each decoder will have decoder blocks. Take the first and the last decoder block.\n",
    "- In each decoder block, you should have a cross-attention layer, and in each cross-attention layer, you should have an attention map for each head, which you would have initialized in `self.attn` class-level variable using the second return value of `attention` function.\n",
    "- You essentially need to do a forward pass using the german sentence (after tokenizing) and the english sentence (after tokenizing), and take this attention map for the above two layers. The output should be heatmap plot of `german_sequence_length x english_sequence_length` for each head in each layer. You can use `seaborn` or `matplotlib` for this visualization.\n",
    "\n",
    "You can add as many code or markdown cells as you need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ff2662b471262",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the visualization of the attention weights for the cross-attention layer\n",
    "# YOUR CODE HERE\n",
    "# Example Heatmap Generation Code\n",
    "# matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# # Display heatmap using Matplotlib\n",
    "# plt.imshow(matrix, cmap='hot', interpolation='nearest')\n",
    "# plt.colorbar()  # Add a color bar to show the scale\n",
    "# plt.title('Heatmap using Matplotlib')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53074653e10365eb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f045bb7b489220",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.3. Distribution of k in Top-p Sampling [5 points - Non-Programming]\n",
    "\n",
    "Extend implementation of top-p sampling to identify the distribution of number of samples (k) when using:\n",
    "- p = 0.5\n",
    "- p= 0.8\n",
    "- p = 0.9\n",
    "\n",
    "Write your code below and print the mean and standard deviation of k in all the three cases using the validation set. Use your existing trained model for this analysis.\n",
    "\n",
    "Note - Your results should execute from the code, and not just written in the form of markdown cells. You can add more code cells if you want.\n",
    "\n",
    "Also, in a markdown cell, provide intuitive reasoning why the output is coming that way.\n",
    "\n",
    "**How to do this?**\n",
    "- Copy the existing `generate` and `sample_top_p` functions from `sequence_generator.py` to the notebook.\n",
    "- Modify the `sample_top_p` function here to also return the number of tokens selected after the candidate tokens are sampled by making prob of tokens that don't meet the criteria to be 0.\n",
    "- Iterate over your validation set.\n",
    "- Collect the value of k for each call to `sample_top_p` and calculate the mean and standard deviation of that distribution.\n",
    "\n",
    "You can add as many code or markdown cells as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278cbf9fa7424e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T02:13:50.461350Z",
     "start_time": "2024-10-03T02:13:50.399379Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_distribution_using_top_p(p=None):\n",
    "    mean = None\n",
    "    std = None\n",
    "    # TODO: Implement the function to calculate the distribution of k for top-p sampling\n",
    "    # YOUR CODE HERE (you can also add more cells, or code outside this boundary)\n",
    "\n",
    "    # END YOUR CODE\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039e730ce958bc2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_p1, std_p1 = k_distribution_using_top_p(0.5)\n",
    "mean_p2, std_p2 = k_distribution_using_top_p(0.8)\n",
    "mean_p3, std_p3 = k_distribution_using_top_p(0.9)\n",
    "\n",
    "print(f'Mean and Standard Deviation for p=0.5: {mean_p1}, {std_p1}')\n",
    "print(f'Mean and Standard Deviation for p=0.8: {mean_p2}, {std_p2}')\n",
    "print(f'Mean and Standard Deviation for p=0.9: {mean_p3}, {std_p3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a97b20f55ce1aea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc027188b5427f9c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.4. Distribution of p in Top-k Sampling [5 points - Non-Programming]\n",
    "\n",
    "Extend and reprogram the implementation of top-k sampling to identify the distribution of cumulative probability of the samples (p) filtered when using:\n",
    "- k = 1\n",
    "- k = 3\n",
    "- k = 5\n",
    "\n",
    "- Write your code below and print the mean and standard deviation of p in all the three cases. Use your existing trained model for this analysis.\n",
    "\n",
    "Note - Your results should execute from the code, and not just written in the form of markdown cells. You can add more code cells if you want.\n",
    "\n",
    "Also, in a markdown cell, provide intuitive reasoning why the output is coming that way.\n",
    "\n",
    "**How to do this?**\n",
    "- Copy the existing `generate` and `sample_top_k` functions from `sequence_generator.py` to the notebook.\n",
    "- Modify the `sample_top_k` function here to also return the cumulative probability of the samples selected after the candidate tokens are sampled. This is after selecting top_k tokens and making prob of tokens that don't meet the criteria to be 0, but before renormalizing the probabilities (since after renormalizing, probability will be 1 only).\n",
    "- Iterate over your validation set.\n",
    "- Collect the value of p for each call to `sample_top_k` and calculate the mean and standard deviation of that distribution.\n",
    "\n",
    "You can add as many code or markdown cells as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c772ed315e7d174",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def p_distribution_using_top_k(k=None):\n",
    "    mean = None\n",
    "    std = None\n",
    "    # TODO: Implement the function to calculate the distribution of p for top-k sampling\n",
    "    # YOUR CODE HERE  (you can also add more cells, or code outside this boundary)\n",
    "\n",
    "    # END YOUR CODE\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d584595aa5c1568",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_k1, std_k1 = p_distribution_using_top_k(k=1)\n",
    "mean_k2, std_k2 = p_distribution_using_top_k(k=3)\n",
    "mean_k3, std_k3 = p_distribution_using_top_k(k=5)\n",
    "\n",
    "print(f'Mean and Standard Deviation for k=1: {mean_k1}, {std_k1}')\n",
    "print(f'Mean and Standard Deviation for k=3: {mean_k2}, {std_k2}')\n",
    "print(f'Mean and Standard Deviation for k=5: {mean_k3}, {std_k3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d9ba7bd14954df",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (add more cells as needed)\n",
    "# TODO: Implement Group Query Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd90d7be52e217",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6. Submit Your Assignment\n",
    "\n",
    "This is the end. Congratulations!\n",
    "\n",
    "Now, follow the steps below to submit your homework on Gradescope.\n",
    "\n",
    "### 6.1. Programming\n",
    "\n",
    "The programming will be evaluated through an autograder. Submit the following files on autograder -\n",
    "1. model.py\n",
    "2. datautils.py\n",
    "3. model_training.py\n",
    "4. sequence_generator.py\n",
    "5. utils.py\n",
    "6. (only needed for Bonus points in 4.7) All the CSV files inside `outputs` directory for BLEU score evaluation .\n",
    "\n",
    "### 6.2. Non-Programming\n",
    "\n",
    "The analysis parts will be evaluated manually. For this, export the notebook to a PDF file, and submit it on GradeScope. Please ensure no written code or output is clipped when you create your PDF. One reliable way to do it is first download it as HTML through Jupyter Notebook and then print it to get PDF."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP_HW0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
